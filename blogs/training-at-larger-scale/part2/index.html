<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-GPU training | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 2 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part2/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">Multi-GPU training</h1> <p class="post-description">Chapter 2 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Training at larger scale </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part1/"> Ch 1: The Setup </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part2/"> Ch 2: Multi-GPU training </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part3/"> Ch 3: Bigger data in the cloud </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part4/"> Ch 4: Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part5/"> Ch 5: Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> Ch 6: What Is Next </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <h2 id="1-single--to-multi-gpu-training">1. Single- to Multi-GPU training</h2> <p>Multi-GPU training provides accelerated computing power, meaning faster training once set up correctly. This is necessary when having bigger datasets and larger models.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Multi-GPU training/
├── config/
│   ├── config.yaml
│   └── cli_config.yaml
├── output/
├── src/
│   ├── data/
│   │   ├── __pycache__/
│   │   ├── lightning_datamodule.py
│   │   └── pytorch_dataset.py
│   └── model/
│       ├── __pycache__/
│       ├── lightning_module.py
│       ├── pytorch_model.py
│       ├── pytorch_encoder.py
│       └── pytorch_decoder.py
├── tests/
│   ├── __pycache__/
│   └── test_lightning_parameters.py
├── __pycache__/
├── lightning_train.py
├── lightning_trainer.py
└── requirements.txt
</code></pre></div></div> <p>By default, pytorch deep learning models only utilize a single GPU for training, even if multiple GPUs are available. This of course is not what we want. Multi-GPU training setup for pytorch itself can be quite a pain, which is why we are going to use (Pytorch) Lightning. This is very useful and saves you a lot of work down the line (not just multi-GPU training), you’ll see.</p> <div align="center"> <img src="/images/training-blog/idle_gpu.webp" alt="Idle GPU Utilization" width="400"> <p><em>When your expensive GPUs sit idle while only one is working, you're wasting resources and time.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <h3 id="strategies">Strategies</h3> <hr> <p>For multi-GPU training, there are a few strategies that can be used to train models. I will list a few below:</p> <p><strong>(Distributed) Data Paralel</strong></p> <p>Replicate the model across all GPUs. Divide the input data into smaller subsets, and assign each subset to a different GPU. The data is only shuffled within the subset, not across the subsets. Each GPU processes its batch independently by running a forward and backward pass using its own model replica. After the backward pass, the gradients from all GPUs are synchronized and averaged. These averaged gradients are then used to update the model parameters consistently across all replicas. This strategy is commonly used because it’s relatively straightforward and scales well across multiple GPUs.</p> <div align="center"> <img src="/images/training-blog/data_parallel.webp" alt="Data Parallel Training" width="400"> </div> <div align="center"> <img src="/images/training-blog/data_parallel_2.png" alt="Data Parallel Training" width="400"> <p><em>Data Parallel training splits the data across GPUs, with each GPU processing a different batch of data.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <p><strong>Model paralel</strong></p> <p>In model parallelism, different parts of the model are placed on different GPUs. For example, GPU 0 might hold the encoder, while GPU 1 holds the decoder. This approach is primarily used when the model is too large to fit into the memory of a single GPU, which is typically the case for models with billions of parameters. Unlike data parallelism where each GPU maintains a full model replica, model parallelism distributes the model across GPUs with each GPU holding only a portion of the model. During training, data (activations) must flow between GPUs during both forward and backward passes, which can introduce potential communication bottlenecks.</p> <p>This strategy is more complex to implement and debug and is usually reserved for advanced scenarios like training large-scale transformers (e.g., GPT-style models).</p> <div align="center"> <img src="/images/training-blog/model_parallel.webp" alt="Model Parallel Training" width="400"> <p><em>Model Parallel training splits the model across GPUs, with each GPU handling different layers of the network.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <p>Another strategy is to use a combination of data parallelism and model parallelism: pipeline parallelism. This is <em>not</em> typically needed for standard model training — not recommended for now unless working with extremely large architectures.</p> <h3 id="pytorch-lightning">(Pytorch) Lightning</h3> <hr> <p>In short: (Pytorch) Lightning is a wrapper around pytorch that can automatically handle multi-gpu communication along with a lot more nice stuff. The key benefit is that Lightning handles all the complexity while still allowing you to customize any part if needed. You get production-ready features without writing boilerplate code, and your code remains clean and focused on the model architecture and training logic. I deeply encourage you to start using this as it will save you a lot of time and effort down the line (At the end of this chapter, I’ll state the advantages of this Lightning so you understand why it is so nice), I will now walk you through how to use lightning:</p> <p><strong>How to go from pytorch to (pytorch) lightning</strong></p> <p>This is actually quite easy. Note that in the <a href="/blogs/training-at-larger-scale/part1/">previous chapter</a> I named all of my components files <code class="language-plaintext highlighter-rouge">pytorch_*.py</code>. This was chosen deliberately, because now I can show you that you just have to add some <code class="language-plaintext highlighter-rouge">lightning_*.py</code> files to make full use of lightning’s benefits. Examples of these can be found in the <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">src</code></a> folder.</p> <p><strong><em><a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">model/lightning_module.py</code></a></em></strong></p> <ul> <li>NOTE: the class AutoencoderModule(L.LightningModule) inherits from the L.LightningModule</li> <li>self.save_hyperparameters() saves all the init arguments as hyperparameters</li> <li>imports your (custom) model</li> <li>logs everything automatically with self.log</li> <li>wraps all of this nicely for Lightning use.</li> <li>Lightning needs the following functions implemented here: <ul> <li>forward()</li> <li>training_step()</li> <li>validation_step()</li> <li>configure_optimizers()</li> </ul> </li> </ul> <p><strong><em><a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">data/lightning_datamodule.py</code></a></em></strong></p> <ul> <li>NOTE: the class DummyDataModule(L.LightningDataModule) inherits from the L.LightningDatamodule</li> <li>self.save_hyperparameters() saves all the init arguments as hyperparameters</li> <li>imports your (custom) Dataset</li> <li>creates Dataloader objects</li> <li>wraps all of this nicely for Lightning use.</li> <li>Lightning needs the following things: <ul> <li>setup()</li> <li>train_loader()</li> <li>val_loader()</li> <li>test_loader()</li> </ul> </li> </ul> <p><strong><em><a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_train.py</code></a></em></strong></p> <ul> <li>NOTE: look how much less code this is! pytorch_train.py had almost 200 lines of code, we now have 74.</li> <li>go through the code and see how everything is initiated.</li> <li>especially important here is the L.trainer() with the following parameters that make multi-GPU implementation super easy <ul> <li> <strong>accelerator</strong>: Specifies the hardware to use (e.g. “auto”, “gpu”, “cpu”, “tpu”). It directs Lightning to use the appropriate backend for accelerated computing.</li> <li> <strong>device</strong>: Indicates which (or the number) device(s) to run on. For example, it could be an integer (like 1 or 4) or a specific device string (e.g., “cuda:0”) or “auto” to choose one or multiple GPUs. If you have 8 GPU’s, use 8.</li> <li> <strong>strategy</strong>: Defines the parallelization approach. Options include “dp” (data parallel), “ddp” (distributed data parallel), “auto”, among others, which determine how training is distributed across GPUs.</li> <li> <strong>num_nodes</strong>: Specifies the number of nodes (or machines) to use for training. (this is relevant when you have multiple clusters)</li> </ul> </li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> unittest tests/test_lightning_parameters.py
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python lightning_train.py
</code></pre></div></div> <p><strong><em><a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_trainer.py</code></a></em></strong></p> <ul> <li>NOTE: even less code and a CLI (very useful because now you can use the –help flag to see everything that can be initiated and how)</li> <li>go through the code and see how everything is initiated. It is important to note that CLI requires a specific format for the <code class="language-plaintext highlighter-rouge">config.yaml</code> (example in <code class="language-plaintext highlighter-rouge">cli_config.yaml</code>) with specific config sections: <ul> <li>model</li> <li>data</li> <li>trainer</li> </ul> </li> </ul> <p>Run the following commands to see the benefits: (note that you can switch between fit, validate, test, predict)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> lightning_trainer <span class="nt">--help</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> lightning_trainer fit <span class="nt">--help</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> lightning_trainer fit <span class="nt">-c</span> config/cli_config.yaml
</code></pre></div></div> <p>NOTE: Additional information on how to use the Lightning CLI is available in the <a href="https://lightning.ai/docs/pytorch/2.1.0/cli/lightning_cli.html" rel="external nofollow noopener" target="_blank">Lightning CLI Documentation</a></p> <h3 id="managing-multiple-processes-in-distributed-training">Managing Multiple Processes in Distributed Training</h3> <hr> <p>When transitioning from single-GPU to multi-GPU training, you need to carefully consider how processes interact with shared resources. This is a critical aspect that can cause subtle bugs if not handled properly.</p> <h4 id="single-process-vs-multiple-processes">Single Process vs. Multiple Processes</h4> <p><strong>Single-GPU Training:</strong></p> <ul> <li>One main process controls everything</li> <li>This process manages the GPU, spawns dataloader workers</li> <li>Handles all file operations (downloads, checkpoints, logs)</li> </ul> <p><strong>Multi-GPU Training:</strong></p> <ul> <li>Multiple processes are spawned (one per GPU)</li> <li>Initially, all processes think they’re the “main” process</li> <li>No hierarchy exists until PyTorch Lightning initializes it</li> <li>Multiple processes might try to access the same files or resources simultaneously, causing conflicts or corrupted data</li> </ul> <h3 id="handling-common-operations">Handling Common Operations</h3> <hr> <h3 id="handling-file-downloads-in-distributed-training">Handling File Downloads in Distributed Training</h3> <p>When training with multiple GPUs, proper coordination of file operations is critical. In <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">pytorch_dataset.py</code></a> you can see how to implement distributed file downloads correctly:</p> <ul> <li>Only main process (rank 0) downloads data</li> <li> <code class="language-plaintext highlighter-rouge">dist.barrier()</code> synchronizes all processes (meaning the other processes have to wait before the main process finishes)</li> <li>Works in both distributed and non-distributed setups</li> <li>Prevents read/write file corruption and redundant downloads</li> </ul> <h4 id="example-implementation">Example Implementation</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_main_process</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Check if the current process is the main process.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">()</span> <span class="ow">or</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">download_dummy_data</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Download the dummy data, only on the main process.</span><span class="sh">"""</span>
    <span class="n">should_download</span> <span class="o">=</span> <span class="nf">is_main_process</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">should_download</span><span class="p">:</span>
        <span class="c1"># Only process with rank 0 performs the download
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Main process downloading data...</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># Actual download code here
</span>    
    <span class="c1"># Synchronization point - all processes must wait here 
</span>    <span class="c1"># Until the main process has finished downloading.
</span>    <span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">():</span>
        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>
        
</code></pre></div></div> <h3 id="handling-file-uploads-in-distributed-training">Handling File Uploads in Distributed Training</h3> <p>When training is complete, uploading checkpoints and logs requires similar coordination to prevent conflicts. In <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_trainer.py</code></a> and <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_train.py</code></a> you can see how to implement distributed file uploads correctly:</p> <ul> <li>Single Process Uploads: Only rank 0 process handles uploads</li> <li>Prevents Conflicts: Eliminates race conditions and redundant operations</li> <li>Efficient: Reduces network traffic and ensures clean termination</li> <li>No need for other processes to wait, they can terminate.</li> </ul> <h4 id="example-implementation-1">Example Implementation</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_main_process</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Check if the current process is the main process.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">()</span> <span class="ow">or</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">upload_checkpoints_to_cloud</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Upload checkpoints and logs to cloud storage, only from the main process.</span><span class="sh">"""</span>
    <span class="c1"># Only process with rank 0 performs the upload
</span>    <span class="k">if</span> <span class="nf">is_main_process</span><span class="p">():</span>
        <span class="c1"># Upload files
</span>        
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Upload complete!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Seeding in Pytorch Lightning</strong> When using PyTorch Lightning’s <code class="language-plaintext highlighter-rouge">seed_everything()</code>, it’s important to note that by default, the seed is (as far as I have tested) not automatically propagated to worker processes or DataLoader generators. Setting <code class="language-plaintext highlighter-rouge">workers=True</code> in <code class="language-plaintext highlighter-rouge">seed_everything()</code> ensures the seed is properly propagated to worker processes, but you still need to explicitly set seeds for DataLoader generators. As far as I have tested (please correct me if I am wrong), in the CLI config you cannot specify workers=True, which is why I also made a custom <code class="language-plaintext highlighter-rouge">worker_init_fn</code> as an example. All of this can be seen in <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_datamodule.py</code></a>. It ensures reproducibility across all components of your Lightning pipeline.</p> <p><strong>Monitoring Multi-GPU Training</strong></p> <p>When training with multiple GPUs, it’s essential to verify that the learning behavior matches expectations. Here’s how to effectively monitor multi-GPU training:</p> <ol> <li> <p><strong>Use a Logging Framework</strong>: Tools like Weights &amp; Biases (WandB), TensorBoard, or MLflow provide visualizations of training metrics across runs.</p> </li> <li> <p><strong>Run Controlled Experiments</strong>: Compare identical configurations between single-GPU and multi-GPU runs for a fixed number of epochs to identify any discrepancies.</p> </li> <li> <p><strong>Hardware Configuration Verification</strong>: My implementation includes a configuration check in <a href="/blogs/training-at-larger-scale/part2/"><code class="language-plaintext highlighter-rouge">lightning_trainer.py</code></a> that prints the actual hardware setup being used:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Training Configuration Check:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Accelerator: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">accelerator</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Devices: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">device_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Strategy: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">strategy</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> </ol> <p><strong>Common Causes of Performance Differences</strong></p> <p>When comparing single-GPU to multi-GPU training, several factors can cause differences in results:</p> <ol> <li> <p><strong>Effective Batch Size</strong>: In data parallel training, the batch size is effectively multiplied by the number of GPUs in your node (or number of GPU’s per node * number of nodes). With 4 GPUs and a batch size of 32, your effective batch size becomes 128, which affects:</p> <ul> <li> <p><strong>Learning Rate Scaling</strong>: You typically need to scale the learning rate linearly with the batch size (the “linear scaling rule”). For example, when moving from 1 to 4 GPUs, consider increasing your learning rate by 4x.</p> </li> <li> <p><strong>Batch Normalization Statistics</strong>: Larger effective batch sizes produce different batch statistics, affecting model convergence.</p> </li> </ul> </li> <li> <strong>Learning Rate Schedulers</strong>: With fewer steps per epoch in multi-GPU training, schedulers need adjustment: <ul> <li>Recalculate <code class="language-plaintext highlighter-rouge">total_steps</code> for step-based schedulers</li> <li>Consider epoch-based schedulers for more consistent behavior</li> <li>Adjust warm up periods proportionally to the effective batch size</li> </ul> </li> <li> <p><strong>Gradient Synchronization</strong>: Different synchronization strategies can affect weight updates and convergence patterns.</p> </li> <li> <strong>Hardware Configuration</strong>: Always explicitly set <code class="language-plaintext highlighter-rouge">strategy</code>, <code class="language-plaintext highlighter-rouge">accelerator</code>, and <code class="language-plaintext highlighter-rouge">devices</code> parameters rather than relying on “auto” settings to ensure consistent behavior across environments.</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lightning_train.py
</code></pre></div></div> <p>Great! now we can train with multiple GPUs, let’s tackle working with <a href="/blogs/training-at-larger-scale/part3/">bigger data in the cloud</a></p> <h3 id="appendix-overview-of-advantages-of-lightning-over-raw-pytorch">Appendix: Overview of advantages of Lightning over Raw PyTorch</h3> <hr> <table> <thead> <tr> <th>Feature</th> <th>PyTorch</th> <th>Lightning</th> </tr> </thead> <tbody> <tr> <td><strong>Device/GPU Handling</strong></td> <td>Manual device placement (<code class="language-plaintext highlighter-rouge">.to(device)</code>), manual multi-GPU management</td> <td> <code class="language-plaintext highlighter-rouge">accelerator="auto"</code>, <code class="language-plaintext highlighter-rouge">devices="auto"</code> handles single GPU, multi-GPU (DDP), TPU, CPU fallback</td> </tr> <tr> <td><strong>Training Loop</strong></td> <td>Manual train/eval mode, gradient zeroing, loss, backward, optimizer steps</td> <td>Handled in <code class="language-plaintext highlighter-rouge">training_step</code>, <code class="language-plaintext highlighter-rouge">validation_step</code> </td> </tr> <tr> <td><strong>Logging</strong></td> <td>Manual <code class="language-plaintext highlighter-rouge">print</code> or custom logging</td> <td>Built-in TensorBoard, WandB, MLflow, auto metric aggregation, progress bars</td> </tr> <tr> <td><strong>Callbacks</strong></td> <td>Manual implementation</td> <td>Built-in model checkpointing, early stopping, learning rate monitoring</td> </tr> <tr> <td><strong>Multi-GPU Strategies</strong></td> <td>Manual data parallelism, distributed training, gradient synchronization</td> <td>Specify <code class="language-plaintext highlighter-rouge">strategy</code> parameter in Trainer</td> </tr> <tr> <td><strong>Profiling &amp; Debugging</strong></td> <td>Manual profiling setup</td> <td>Built-in profilers and debugging tools</td> </tr> <tr> <td><strong>Reproducibility</strong></td> <td>Manual seed setting everywhere</td> <td> <code class="language-plaintext highlighter-rouge">seed_everything()</code> (Note: manual seed for data loader/workers still needed)</td> </tr> <tr> <td><strong>Mixed Precision Training</strong></td> <td>Manual AMP implementation</td> <td> <code class="language-plaintext highlighter-rouge">precision="16-mixed"</code> in Trainer</td> </tr> <tr> <td><strong>Automatic Sanity Checks</strong></td> <td>No built-in pre-training validation</td> <td>Sanity validation batch (model forward pass, shape compatibility, loss calculation). Disable with <code class="language-plaintext highlighter-rouge">num_sanity_val_steps=0</code>.</td> </tr> <tr> <td><strong>Cloud Storage Integration</strong></td> <td>Manual cloud storage uploads</td> <td>Easy integration via custom callbacks for checkpoint uploads (e.g., AWS S3, GCS). Prevents data loss, customizable backup strategies.</td> </tr> </tbody> </table> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>