<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-GPU training | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 2 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part2/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">Multi-GPU training</h1> <p class="post-description">Chapter 2 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Introduction </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part1/"> Ch 1: The Setup </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part2/"> Ch 2: Multi-GPU training </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part3/"> Ch 3: Bigger data in the cloud </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part4/"> Ch 4: Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part5/"> Ch 5: Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> Ch 6: What Is Next </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <h2 id="1-single--to-multi-gpu-training">1. Single- to Multi-GPU training</h2> <p>Multi-GPU training provides accelerated computing power, meaning faster training once setup correctly. This is necessary when having bigger datasets and larger models.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Multi-GPU training/
├── config/
│   ├── config.yaml
│   └── cli_config.yaml
├── output/
├── src/
│   ├── data/
│   │   ├── __pycache__/
│   │   ├── lightning_datamodule.py
│   │   └── pytorch_dataset.py
│   └── model/
│       ├── __pycache__/
│       ├── lightning_module.py
│       ├── pytorch_model.py
│       ├── pytorch_encoder.py
│       └── pytorch_decoder.py
├── tests/
│   ├── __pycache__/
│   └── test_lightning_parameters.py
├── __pycache__/
├── lightning_train.py
├── lightning_trainer.py
└── requirements.txt
</code></pre></div></div> <p>By default, pytorch deep learning models only utilize a single GPU for training, even if multiple GPUs are available. This ofcourse is not what we want. Multi-GPU training setup for pytorch itself can be quite a pain, which is why we are going to use (Pytorch) Lightning. This is very useful and saves you a lot of work down the line (not just multi GPU stuff), you’ll see.</p> <p>TODO: image and source/text format nicely <img src="/images/training-blog/idle_gpu.webp" alt="Idle GPU Utilization"> <em>When your expensive GPUs sit idle while only one is working, you’re wasting resources and time.</em> SOURCE: https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training</p> <h3 id="strategies">Strategies</h3> <hr> <p>When we have multiple GPU’s there are a few strategies that can be used to train models. I will list a few below:</p> <p><strong>(Distributed) Data parralel</strong></p> <p>Replicate the model across all GPUs. Divide the input data into smaller subsets, and assign each subset to a different GPU. NOTE: the data is only shuffled within the subset, not across the subsets. (all is Multi-process) Each GPU processes its batch independently by running a forward and backward pass using its own model replica. After the backward pass, the gradients from all GPUs are synchronized and averaged. These averaged gradients are then used to update the model parameters consistently across all replicas. This strategy is commonly used because it’s relatively straightforward and scales well across multiple GPUs.</p> <div align="center"> <img src="images/data_parallel.webp" alt="Data Parallel Training" width="400"> <img src="images/data_parallel_2.png" alt="Data Parallel Training" width="400"> <p><em>Data Parallel training splits the data across GPUs, with each GPU processing a different batch of data.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <p><strong>Model parralel</strong></p> <p>In model parallelism, different parts of the model are placed on different GPUs. For example, GPU 0 might hold the encoder, while GPU 1 holds the decoder. • Primarily used when the model is too large to fit into the memory of a single GPU (typically models with billions of parameters). • Unlike data parallelism, where each GPU has a full model replica, here each GPU holds only a portion of the model. • Training requires data (activations) to flow between GPUs during forward and backward passes, introducing potential communication bottlenecks.</p> <p>This strategy is more complex to implement and debug and is usually reserved for advanced scenarios like training large-scale transformers (e.g., GPT-style models). Not typically needed for standard model training — not recommended for now unless working with extremely large architectures.</p> <p>NOTE: INCORPORATE? You may also encounter “sharded model parallelism” (or pipeline parallelism), which are more scalable versions used in massive model training setups.</p> <div align="center"> <img src="images/model_parallel.webp" alt="Model Parallel Training" width="400"> <p><em>Model Parallel training splits the model across GPUs, with each GPU handling different layers of the network.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <p><strong>pipeline parralel</strong></p> <p>This is often considered a combination of data parallelism and model parallelism.<br> Not typically needed for standard model training — not recommended for now unless working with extremely large architectures.</p> <div align="center"> <img src="images/pipeline_parallel.webp" alt="Pipeline Parallel Training" width="400"> <p><em>Pipeline Parallel training processes data in stages across GPUs, similar to an assembly line.</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a>, <a href="https://www.dailydoseofds.com/a-beginner-friendly-guide-to-multi-gpu-model-training/" rel="external nofollow noopener" target="_blank">A Beginner-Friendly Guide to Multi-GPU Model Training</a></small> </div> <h3 id="pytorch-lightning">(Pytorch) Lightning</h3> <hr> <p>In short: (Pytorch) Lightning is a wrapper around pytorch that can automatically handle multi-gpu communication along with a lot more nice stuff. The key benefit is that Lightning handles all the complexity while still allowing you to customize any part if needed. You get production-ready features without writing boilerplate code, and your code remains clean and focused on the model architecture and training logic. I deeply encourage you to start using this as it will save you a lot of time and effort down the line (At the end of this chapter, I’ll state the advantages of this Lightning so you understand why it is so nice), I will now walk you through how to use lightning:</p> <p><strong>How to go from pytorch to (pytorch) lightning</strong></p> <p>This is actually quite easy. Note that earlier I named all of my components files <code class="language-plaintext highlighter-rouge">pytorch_*.py</code>. This was chosen deliberately, because now I can show you that you just have to add some <code class="language-plaintext highlighter-rouge">lightning_*.py</code> files to make full use of lightning’s benefits. Examples of these can be found in the <code class="language-plaintext highlighter-rouge">src</code> folder.</p> <p><strong><em>model/lightning_module.py</em></strong></p> <ul> <li>NOTE: the class AutoencoderModule(L.LightningModule) inherits from the L.LightningModule</li> <li>self.save_hyperparameters() saves all the init arguments as hyperparameters</li> <li>imports your (custom) model</li> <li>logs everything automatically with self.log</li> <li>wraps all of this nicely for Lightning use.</li> <li>Lightning needs the following functions implemented here: <ul> <li>forward()</li> <li>training_step()</li> <li>validation_step()</li> <li>configure_optimizers()</li> </ul> </li> </ul> <p><strong><em>data/lightning_datamodule.py</em></strong></p> <ul> <li>NOTE: the class DummyDataModule(L.LightningDataModule) inherits from the L.LightningDatamodule</li> <li>self.save_hyperparameters() saves all the init arguments as hyperparameters</li> <li>imports your (custom) Dataset</li> <li>creates Dataloader objects</li> <li>wraps all of this nicely for Lightning use.</li> <li>Lightning needs the following things: <ul> <li>setup()</li> <li>train_loader()</li> <li>val_loader()</li> <li>test_loader()</li> </ul> </li> </ul> <p><strong><em>lightning_train.py</em></strong></p> <ul> <li>NOTE: look how much less code this is! pytorch_train.py had almost 200 lines of code, we now have 74.</li> <li>go through the code and see how everything is initiated.</li> <li>especially important here is the L.trainer() with the following parameters that make multi-GPU implementation super easy <ul> <li> <strong>accelerator</strong>: Specifies the hardware to use (e.g. “auto”, “gpu”, “cpu”, “tpu”). It directs Lightning to use the appropriate backend for accelerated computing.</li> <li> <strong>device</strong>: Indicates which (or the number) device(s) to run on. For example, it could be an integer (like 1 or 4) or a specific device string (e.g., “cuda:0”) or “auto” to choose one or multiple GPUs. If you have 8 GPU’s, use 8.</li> <li> <strong>strategy</strong>: Defines the parallelization approach. Options include “dp” (data parallel), “ddp” (distributed data parallel), “auto”, among others, which determine how training is distributed across GPUs.</li> </ul> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m unittest tests/test_lightning_parameters.py
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lightning_train.py
</code></pre></div></div> <p><strong><em>lightning_trainer.py</em></strong></p> <ul> <li>NOTE: even less code and a CLI (very useful because now you can use the –help flag to see everything that can be initiated and how)</li> <li>go through the code and see how everything is initiated. It is important to note that CLI requires a specific format for the <code class="language-plaintext highlighter-rouge">config.yaml</code> (example in <code class="language-plaintext highlighter-rouge">cli_config.yaml</code>) with specific config sections: <ul> <li>model</li> <li>data</li> <li>trainer</li> <li>TODO OPTIMIZER SCHEDULER????</li> </ul> </li> </ul> <p>Run the following commands to see the benefits: (note that you can switch between fit, validate, test, predict)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m lightning_trainer --help
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m lightning_trainer fit --help
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m lightning_trainer fit -c config/cli_config.yaml
</code></pre></div></div> <p>NOTE: more reading and how to use it in: <a href="https://lightning.ai/docs/pytorch/2.1.0/cli/lightning_cli.html" rel="external nofollow noopener" target="_blank">Lightning CLI Documentation</a></p> <h3 id="managing-multiple-processes-in-distributed-training">Managing Multiple Processes in Distributed Training</h3> <hr> <p>When transitioning from single-GPU to multi-GPU training, you need to carefully consider how processes interact with shared resources. This is a critical aspect that can cause subtle bugs if not handled properly.</p> <h4 id="single-process-vs-multiple-processes">Single Process vs. Multiple Processes</h4> <p><strong>Single-GPU Training:</strong></p> <ul> <li>One main process controls everything</li> <li>This process manages the GPU, spawns dataloader workers</li> <li>Handles all file operations (downloads, checkpoints, logs)</li> </ul> <p><strong>Multi-GPU Training:</strong></p> <ul> <li>Multiple processes are spawned (one per GPU)</li> <li>Initially, all processes think they’re the “main” process</li> <li>No hierarchy exists until PyTorch Lightning initializes it</li> <li>Multiple processes might try to access the same files or resources simultaneously, causing conflicts or corrupted data</li> </ul> <h3 id="handling-common-operations">Handling Common Operations</h3> <hr> <h3 id="handling-file-downloads-in-distributed-training">Handling File Downloads in Distributed Training</h3> <p>When training with multiple GPUs, proper coordination of file operations is critical. Here’s how to implement distributed file downloads correctly:</p> <ul> <li>Only main process (rank 0) downloads data</li> <li> <code class="language-plaintext highlighter-rouge">dist.barrier()</code> synchronizes all processes (meaning the other processes have to wait before the main process finishes)</li> <li>Works in both distributed and non-distributed setups</li> <li>Prevents file corruption and redundant downloads</li> </ul> <h4 id="example-implementation">Example Implementation</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_main_process</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Check if the current process is the main process.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">()</span> <span class="ow">or</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">download_dummy_data</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Download the dummy data, only on the main process.</span><span class="sh">"""</span>
    <span class="n">should_download</span> <span class="o">=</span> <span class="nf">is_main_process</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">should_download</span><span class="p">:</span>
        <span class="c1"># Only process with rank 0 performs the download
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Main process downloading data...</span><span class="sh">"</span><span class="p">)</span>
        <span class="c1"># Actual download code here
</span>
    <span class="c1"># Synchronization point - all processes must wait here
</span>    <span class="c1"># Until the main process has finished downloading.
</span>    <span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">():</span>
        <span class="n">dist</span><span class="p">.</span><span class="nf">barrier</span><span class="p">()</span>

</code></pre></div></div> <p>This is also implemented in <code class="language-plaintext highlighter-rouge">pytorch_dataset.py</code></p> <h3 id="handling-file-uploads-in-distributed-training">Handling File Uploads in Distributed Training</h3> <p>When training is complete, uploading checkpoints and logs requires similar coordination to prevent conflicts. Here’s how to implement distributed file uploads correctly:</p> <ul> <li>Single Process Uploads: Only rank 0 process handles uploads</li> <li>Prevents Conflicts: Eliminates race conditions and redundant operations</li> <li>Efficient: Reduces network traffic and ensures clean termination</li> <li>No need for other processes to wait, they can terminate.</li> </ul> <h4 id="example-implementation-1">Example Implementation</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_main_process</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Check if the current process is the main process.</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">()</span> <span class="ow">or</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">upload_checkpoints_to_cloud</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Upload checkpoints and logs to cloud storage, only from the main process.</span><span class="sh">"""</span>
    <span class="c1"># Only process with rank 0 performs the upload
</span>    <span class="k">if</span> <span class="nf">is_main_process</span><span class="p">():</span>
        <span class="c1"># Upload files
</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Upload complete!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This is also implemented in <code class="language-plaintext highlighter-rouge">lightning_trainer.py</code> and <code class="language-plaintext highlighter-rouge">lightning_train.py</code></p> <p><strong>reproducibility</strong></p> <p>When using PyTorch Lightning’s <code class="language-plaintext highlighter-rouge">seed_everything()</code>, it’s important to note that by default, the seed is (as far as I have tested) not automatically propagated to worker processes or DataLoader generators. This can lead to unexpected behavior, especially in multi-GPU training scenarios. Setting <code class="language-plaintext highlighter-rouge">workers=True</code> in <code class="language-plaintext highlighter-rouge">seed_everything()</code> ensures the seed is properly propagated to worker processes, but you still need to explicitly set seeds for DataLoader generators. As far as I have tested (please correct me if I am wrong), in the CLI config you cannot specify workers=True, which is why I also made a custom worker_init_fn as an example. All of this can be seen in <code class="language-plaintext highlighter-rouge">1. Multi-GPU training/src/data/lightning_datamodule.py</code>. It ensures reproducibility across all components of your training pipeline, including data loading and augmentation steps that happen in worker processes.</p> <p><strong>Monitoring Multi-GPU Training</strong></p> <p>When training with multiple GPUs, it’s essential to verify that the learning behavior matches expectations. Here’s how to effectively monitor multi-GPU training:</p> <ol> <li> <p><strong>Use a Logging Framework</strong>: Tools like Weights &amp; Biases (WandB), TensorBoard, or MLflow provide visualizations of training metrics across runs.</p> </li> <li> <p><strong>Run Controlled Experiments</strong>: Compare identical configurations between single-GPU and multi-GPU runs for a fixed number of epochs to identify any discrepancies.</p> </li> <li> <p><strong>Hardware Configuration Verification</strong>: My implementation includes a configuration check in <code class="language-plaintext highlighter-rouge">1. Multi-GPU training/lightning_trainer.py</code> that prints the actual hardware setup being used:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Training Configuration Check:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Accelerator: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">accelerator</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Devices: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">device_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">- Strategy: </span><span class="si">{</span><span class="n">trainer</span><span class="p">.</span><span class="n">strategy</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> </ol> <p><strong>Common Causes of Performance Differences</strong></p> <p>When comparing single-GPU to multi-GPU training, several factors can cause differences in results:</p> <ol> <li> <p><strong>Effective Batch Size</strong>: In data parallel training, the batch size is effectively multiplied by the number of GPUs. With 4 GPUs and a batch size of 32, your effective batch size becomes 128, which affects:</p> <ul> <li> <strong>Learning Rate Scaling</strong>: You typically need to scale the learning rate linearly with the batch size (the “linear scaling rule”). For example, when moving from 1 to 4 GPUs, consider increasing your learning rate by 4x.</li> <li> <strong>Batch Normalization Statistics</strong>: Larger effective batch sizes produce different batch statistics, affecting model convergence.</li> </ul> </li> <li> <p><strong>Learning Rate Schedulers</strong>: With fewer steps per epoch in multi-GPU training, schedulers need adjustment:</p> <ul> <li>Recalculate <code class="language-plaintext highlighter-rouge">total_steps</code> for step-based schedulers</li> <li>Consider epoch-based schedulers for more consistent behavior</li> <li>Adjust warmup periods proportionally to the effective batch size</li> </ul> </li> <li> <p><strong>Gradient Synchronization</strong>: Different synchronization strategies can affect weight updates and convergence patterns.</p> </li> <li> <p><strong>Hardware Configuration</strong>: Always explicitly set <code class="language-plaintext highlighter-rouge">strategy</code>, <code class="language-plaintext highlighter-rouge">accelerator</code>, and <code class="language-plaintext highlighter-rouge">devices</code> parameters rather than relying on “auto” settings to ensure consistent behavior across environments.</p> </li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python lightning_train.py
</code></pre></div></div> <p>Great! now we can train with multiple GPUs, let’s tackle working with <a href="/blogs/training-at-larger-scale/part3/">bigger data in the cloud</a></p> <h3 id="appendix-overview-of-advantages-of-lightning-over-raw-pytorch">Appendix: Overview of advantages of Lightning over Raw PyTorch</h3> <hr> <p><strong>1. Automatic Device/GPU Handling</strong></p> <ul> <li> <strong>PyTorch</strong>: You need to manually handle device placement with <code class="language-plaintext highlighter-rouge">.to(device)</code> and manage multi-GPU training yourself</li> <li> <strong>Lightning</strong>: Just specify <code class="language-plaintext highlighter-rouge">accelerator="auto"</code> and <code class="language-plaintext highlighter-rouge">devices="auto"</code> and Lightning handles: <ul> <li>Single GPU training</li> <li>Multi-GPU training (automatically using DistributedDataParallel)</li> <li>TPU support</li> <li>CPU fallback</li> </ul> </li> </ul> <p><strong>2. Training Loop Abstractions</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual implementation of: <ul> <li>Train/eval mode switching</li> <li>Gradient zeroing</li> <li>Loss calculation</li> <li>Backward passes</li> <li>Optimizer steps</li> </ul> </li> <li> <strong>Lightning</strong>: All handled automatically in the <code class="language-plaintext highlighter-rouge">training_step</code> and <code class="language-plaintext highlighter-rouge">validation_step</code> </li> </ul> <p><strong>3. Built-in Logging</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual print statements or custom logging implementation</li> <li> <strong>Lightning</strong>: Built-in support for: <ul> <li>TensorBoard</li> <li>WandB</li> <li>MLflow</li> <li>Automatic metric aggregation</li> <li>Progress bars</li> </ul> </li> </ul> <p><strong>4. Callbacks System</strong></p> <ul> <li> <strong>PyTorch</strong>: Need to implement everything manually</li> <li> <strong>Lightning</strong>: Built-in callbacks for: <ul> <li>Model checkpointing</li> <li>Early stopping</li> <li>Learning rate monitoring</li> </ul> </li> </ul> <p><strong>5. Multi-GPU Strategies</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual implementation of: <ul> <li>Data parallelism</li> <li>Distributed training</li> <li>Gradient synchronization</li> </ul> </li> <li> <strong>Lightning</strong>: Just specify the strategy parameter in the Trainer</li> </ul> <p><strong>6. Profiling and Debugging</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual profiling setup</li> <li> <strong>Lightning</strong>: Built-in profilers and debugging tools</li> </ul> <p><strong>7. Reproducibility</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual seed setting everywhere</li> <li> <strong>Lightning</strong>: Global seed management with <code class="language-plaintext highlighter-rouge">seed_everything()</code> <ul> <li>This does not work for the dataloader (and workers), you need to manually set the seed for the dataloader generator (and workers)</li> </ul> </li> </ul> <p><strong>8. Mixed Precision Training</strong></p> <ul> <li> <strong>PyTorch</strong>: Manual AMP (Automatic Mixed Precision) implementation</li> <li> <strong>Lightning</strong>: Just add <code class="language-plaintext highlighter-rouge">precision="16-mixed"</code> to Trainer</li> </ul> <p><strong>9. Automatic Sanity Checks</strong></p> <ul> <li> <strong>PyTorch</strong>: No built-in validation before training</li> <li> <strong>Lightning</strong>: Performs sanity validation batch before training to catch errors early <ul> <li>Validates model forward pass</li> <li>Checks shape compatibility</li> <li>Verifies loss calculation</li> <li>Can be disabled with <code class="language-plaintext highlighter-rouge">num_sanity_val_steps=0</code> if needed</li> </ul> </li> </ul> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>