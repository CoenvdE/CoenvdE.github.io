<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 2. Bigger Data In The Cloud | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 3 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part3/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">2. Bigger Data In The Cloud</h1> <p class="post-description">Chapter 3 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Training at larger scale </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part1/"> Ch 1: 0. The Setup </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part2/"> Ch 2: 1. Single- to Multi-GPU training </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part3/"> Ch 3: 2. Bigger Data In The Cloud </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part4/"> Ch 4: 3. Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part5/"> Ch 5: 4. Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> Ch 6: 5. What's Next? </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <p>In this part, I will provide a general overview of what streaming is and how to work with data in the cloud, I will also provide a usecase specific example for loading and working with geospatial data (xarray) to show an example of with bigger datasets.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2. Bigger data in the cloud/
├── data/
│   ├── usecase_cloud_dataset.py
│   ├── utils.py
│   ├── usecase_API_access.py
│   ├── usecase_cloud_access.py
│   ├── example_cloud_access.py
│   └── __pycache__/
├── output/
│   ├── copernicus_data/
│   └── test/
└── __pycache__/
</code></pre></div></div> <h3 id="quick-recap">Quick Recap:</h3> <p><strong>torch.utils.data.Dataset</strong>: A PyTorch Dataset is a class that tells PyTorch how to get one data sample. It acts as a blueprint for reading a sample from storage (local disk or xloud storage) into active memory (RAM). It has two main functions:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">__len__</code>: how many samples there are</li> <li> <code class="language-plaintext highlighter-rouge">__getitem__</code>: how to get one sample (e.g., load an image, label) Note that this can be from local storage or from the cloud.</li> </ul> <p>The dataset doesn’t load anything. It’s just a blueprint.</p> <p>Cloud data is typically stored on object storage servers like:</p> <ul> <li>Amazon S3</li> <li>Google Cloud Storage (GCS)</li> <li>Azure Blob Storage</li> </ul> <p>NOTE: more reading: <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="external nofollow noopener" target="_blank">PyTorch Dataset Tutorial</a></p> <h2 id="the-problem-datasets-that-are-to-big-for-memory-or-local-disk">The Problem: datasets that are to big for memory or local disk</h2> <p>Most data that you will use lives in “the cloud”: servers somewhere on the planet, with all the data you will ever need. You can access data on the internet and download it to locally (like CIFAR-10, as implemented by pytorch). So what do you do when you have so much data that it does not fit on your local machine?</p> <p>One solution is to buy hard drive and store the data on there. This itself is quite expensive, but could be beneficial if you have the hardware (GPU) yourself. In this case, you can just keep the whole pipeline local, but this is only beneficial you to have (multiple GPU’s), which is super expensive.</p> <p>TODO (With Laurens): add a part of mounting a virtual SSD (or something like that) to the cloud compute instance. and working with that. (pros and cons)</p> <p>Since I don’t have a lot of GPUs (or money), I chose another option: streaming/lazy-loading. Streaming or lazy-loading datasets means loading data only when needed, rather than reading everything into memory upfront. This approach is useful for large datasets that don’t fit in memory or are stored in remote data sources. Instead, you load samples (or chunks of data) on-the-fly as needed, directly from cloud storage servers. This avoids filling up local storage or memory while still giving you access to massive datasets that would otherwise be impossible to work with on limited hardware.</p> <p>Some examples where this happens:</p> <ul> <li>Hugging Face Datasets (streaming=True)</li> <li>WebDataset format (tar files accessed remotely or locally)</li> <li>Some torch.utils.data.Dataset implementations with lazy loading</li> <li>Some API’s provide this functionality</li> </ul> <h3 id="getting-access-to-the-data">Getting access to the data</h3> <p>When you want to stream/lazy-load/get data from cloud storage, you need access to the place it is stored (commonly called buckets). This can sometimes be implemented already by API’s (e.g. Huggingface, or my usecase: Copernicus, see <code class="language-plaintext highlighter-rouge">usecase_API_access.py</code>).</p> <p>In my case, I needed to get access to the cloud storage directly without the API. This direct access gave me more flexibility and control over how I loaded the data. I implemented this using the <code class="language-plaintext highlighter-rouge">fsspec</code> library, which provides a unified interface for working with different file systems and storage backends. This approach was particularly valuable because:</p> <ol> <li>It allowed me to work with multiple storage providers using the same code</li> <li>It enabled parallel data access, significantly improving loading speeds</li> <li>It gave me fine-grained control over chunking and caching strategies</li> <li>It integrated well with my existing PyTorch data pipeline</li> </ol> <p>I’ve created both a general example (<code class="language-plaintext highlighter-rouge">example_cloud_access.py</code>) and a use-case specific (<code class="language-plaintext highlighter-rouge">usecase_cloud_access.py</code>) implementation showing how to access data in the cloud efficiently</p> <p>NOTE (With Laurens): COPERNICUS DIDNT WANT EVERYONE TO KNOW, IS THIS OKAY?</p> <h3 id="streaminglazy-loading">Streaming/Lazy Loading</h3> <p>Now that we have access to the data, we can stream it into memory! For my geospatial data use case, I utilized libraries like Dask and Xarray that provide optimizable, efficient lazy loading capabilities. Dask helps create a computational graph for loading data chunks from storage only when needed, while managing parallel workers to speed up the process. I’ll cover optimization strategies for (streaming) data pipelines in <a href="3.%20Optimizing%20the%20pipeline%3A%20Data.md">Chapter 3: Optimizing the Pipeline: Data</a>. To be able to work with this (lazy-loaded) data from the cloud in your training pipeline, we need to wrap everything into a PyTorch dataset. I’ve implemented an example in <code class="language-plaintext highlighter-rouge">usecase_cloud_dataset.py</code> that demonstrates how to create a custom Dataset class that handles cloud data access, lazy loading and converting it to a usable dataset. Note that this is still a simplified version. When working with Xarray, xbatcher is the most efficient way to use batch generation, but this is out of the scope of this guide. Feel free to ask any questions about this.</p> <h3 id="quick-recap-1">Quick Recap:</h3> <p><strong>torch.utils.data.Dataloader</strong>: A PyTorch DataLoader defines how to load (many) samples (efficiently) (batching, multiprocessing, shuffling, etc.). Basically the DataLoader keeps asking the Dataset (blueprint) for samples, and handles the rest. It:</p> <ul> <li>Wraps a Dataset</li> <li>Loads batches of data (samples) (in parallel using multiple workers)</li> <li>Handles shuffling, batching, prefetching, etc.</li> <li>Dataloader has been used to parallelize the data loading as this boosts up the speed and saves memory.</li> </ul> <p>The Dataloader calls the <strong>getitem</strong>() from the Dataset to get the needed samples. The Dataset (blueprint) defines “what” a sample is and how to get it, the DataLoader defines how to load them efficiently.</p> <h3 id="dataloader-parameters-for-efficient-data-loading-from-the-cloud">DataLoader Parameters for Efficient Data Loading (from the cloud)</h3> <p><strong>Number of Workers (<code class="language-plaintext highlighter-rouge">num_workers</code>)</strong>:</p> <ul> <li>Specifies how many subprocesses should be used to load data. Each of these subprocesses retrieves a batch of data from your dataset and sends it to the main training process. This is not necessarily equal to the number of cores/threads the dataloader uses. Each worker operates independently, loading data in parallel. The loaded data is then sent to the main process(es) for use in training, creating an overlap between training and data loading that reduces idle GPU time. Under the hood, the dataset object is replicated in each worker.</li> <li> <code class="language-plaintext highlighter-rouge">num_workers=0</code>: Data is loaded in the main process. No parallelism.</li> <li> <code class="language-plaintext highlighter-rouge">num_workers=N</code>: Spawns N worker processes to load data in parallel. <ul> <li>Note: This is <strong>not the number of CPU cores</strong>, but the number of subprocesses that utilize CPU resources.</li> </ul> </li> <li>each worker might maintain its own HTTP connection or file handle. This can increase throughput (multiple parallel reads) but also load (e.g., more network connections). Ensure your data source can handle concurrent access.</li> </ul> <p>I will show how to optimize this in the next chapter <a href="3.%20Optimizing%20the%20pipeline%3A%20Data.md">Optimizing the Pipeline: Data</a></p> <p><strong>Persistent Workers (<code class="language-plaintext highlighter-rouge">persistent_workers</code>)</strong>:</p> <ul> <li>If <code class="language-plaintext highlighter-rouge">True</code>, keeps worker processes alive across epochs and thus reduces worker startup overhead.</li> <li>Especially useful when using slower file systems or large datasets as opening/closing files is time expensive.</li> <li>While persistent workers reduce startup overhead, they can lead to increased memory usage over time as workers may accumulate cached data or experience memory leaks. If you notice growing memory consumption during training. In this case you should reduce the number of workers.</li> </ul> <p><strong>Pin Memory (<code class="language-plaintext highlighter-rouge">pin_memory</code>)</strong>:</p> <ul> <li>If <code class="language-plaintext highlighter-rouge">True</code>, enables automatic allocation of fetched tensors into page-locked (pinned) memory. This can significantly speed up host (CPU) to device (GPU) memory transfer. <ul> <li>Normal memory -&gt; GPU: Requires copy to temporary pageable buffer first</li> <li>Pinned memory -&gt; GPU: Direct transfer without intermediate copies</li> </ul> </li> <li>Requires that your dataset’s <code class="language-plaintext highlighter-rouge">__getitem__</code> returns <code class="language-plaintext highlighter-rouge">torch.Tensor</code> objects.</li> </ul> <p><strong>Prefetch Factor (<code class="language-plaintext highlighter-rouge">prefetch_factor</code>)</strong>:</p> <ul> <li>Number of batches loaded in advance by each worker.</li> <li>Total prefetched batches = <code class="language-plaintext highlighter-rouge">num_workers * prefetch_factor</code>. This needs to taken into account for memory consumption</li> <li>Higher values increase memory usage, but can improve throughput by reducing I/O bottlenecks.</li> <li>If using streaming datasets (WebDataset tar files, tf.data pipelines, etc.), ensure you take advantage of their features like prefetching and parallel reads</li> </ul> <p>I will show how to optimize this in the next chapter <a href="3.%20Optimizing%20the%20pipeline%3A%20Data.md">Optimizing the Pipeline: Data</a></p> <p><strong>Multiprocessing Context</strong>: Controls how worker processes are created in the DataLoader (usable with multiple workers):</p> <ul> <li>Use <code class="language-plaintext highlighter-rouge">multiprocessing_context='forkserver'</code> (or <code class="language-plaintext highlighter-rouge">'spawn'</code>) for compatibility with CUDA and complex I/O or filesystem interactions.</li> <li> <code class="language-plaintext highlighter-rouge">'spawn'</code> is the most compatible and is default on Windows and MacOS. Creates entirely new Python processes from scratch, with clean memory space. Safest but slowest method since it needs to re-import modules in each worker.</li> <li> <code class="language-plaintext highlighter-rouge">'forkserver'</code> can also be safer than <code class="language-plaintext highlighter-rouge">'fork'</code> (default on Linux) when using CUDA. Creates a server process that handles forking child processes on demand. Offers a good balance between safety and performance.</li> <li> <code class="language-plaintext highlighter-rouge">'fork'</code> is fast but can lead to subtle bugs with CUDA or file handles. Default on Linux. Creates workers by duplicating the entire parent process memory, including all open resources. Fast but dangerous with complex resources like CUDA contexts or file handles.</li> </ul> <p>When this is not set correctly, you can run into gridlock. This is when worker processes become deadlocked or severely bottlenecked, preventing efficient data flow. Common scenarios include:</p> <ul> <li>CUDA errors when using <code class="language-plaintext highlighter-rouge">'fork'</code> with GPU operations</li> <li>Corrupted file handles when using <code class="language-plaintext highlighter-rouge">'fork'</code> with complex I/O</li> <li>Memory leaks from improper resource sharing</li> <li>Deadlocks from competing access to shared resources</li> </ul> <p><strong>Worker Initialization (<code class="language-plaintext highlighter-rouge">worker_init_fn</code>)</strong>:</p> <ul> <li>Optional function to initialize each worker after it’s spawned.</li> <li>For cloud use: configure filesystem-specific settings and avoid duplication randomness across workers. An example is shown in <code class="language-plaintext highlighter-rouge">utils.py</code> </li> <li>NOTE: Initialization functions are called once per worker process, not once per batch.</li> </ul> <p><strong>Additional Resources:</strong></p> <ul> <li> <a href="https://pytorch.org/docs/stable/data.html" rel="external nofollow noopener" target="_blank">PyTorch DataLoader Documentation</a> - Complete reference for DataLoader parameters and best practices</li> <li> <a href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods" rel="external nofollow noopener" target="_blank">Python Multiprocessing Start Methods</a> - Details on fork, spawn, and forkserver methods and their implications</li> </ul> <p>Now we have a clean architecture setup that is validated and set up for multi-GPU training. We’ve learned how to access and stream data from the cloud, and configured our DataLoader with the appropriate parameters for efficient data loading. With these foundations in place, it’s time to optimize the components of our pipeline for better performance. Let’s start by focusing <a href="3.%20Optimizing%20the%20pipeline%3A%20Data.md">optimizing the data part of the pipeline</a></p> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>