<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Optimizing the pipeline: Model | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 5 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part5/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">Optimizing the pipeline: Model</h1> <p class="post-description">Chapter 5 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Introduction </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part1/"> Ch 1: The Setup </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part2/"> Ch 2: Multi-GPU training </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part3/"> Ch 3: Bigger data in the cloud </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part4/"> Ch 4: Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part5/"> Ch 5: Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> Ch 6: What Is Next </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <h2 id="4-optimizing-the-pipeline-model">4. Optimizing the pipeline: Model</h2> <p>After optimizing the data pipeline, the next step is profiling the model pipeline to catch bottlenecks like slow ops or CPU–GPU data transfers. This is an optional step that, if the code is implemented correctly, will probably not have a big impact on the training time. if you don’t want to do it, you can skip this section and look at <a href="/blogs/training-at-larger-scale/part6/">What Is Next</a>. Note that I give a few nice tools to help you analyse the model pipeline performance. I do suggest you to run the benchmark (and a pytorch profiler) and let ChatGPT or another good LLM analyse the results for you and help you figure out if you need to change something. This part is mainly about getting the time it takes for a batch to pass through your model pipeline down.</p> <p>The time it takes for a batch to pass through your model depends on several factors:</p> <ul> <li> <strong>Batch size</strong>: Larger batch sizes generally increase the duration of a single training step because more data is processed simultaneously. Operations like a dot product scale linearly with the batch size, so the duration of a single training step increases with the batch size. However, the relationship isn’t always linear - GPUs can achieve higher utilization with larger batches, potentially making the per-sample processing time lower.</li> <li> <p><strong>Model complexity</strong>: More complex models (deeper, wider networks) take longer to process each batch.</p> </li> <li> <strong>Hardware</strong>: The specs of your GPU/TPU significantly impact processing time.</li> </ul> <p>TODO (rewrite when discussed with laurens and andy): check if this is correct. In my experiments, a single training step took around 0.4 seconds for moderate-sized models with a batch size of 32. This can vary widely - from milliseconds for small models to several seconds for large transformer architectures. I have seen 0.1 seconds as well with other models.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>4. Optimizing the pipeline: Model/
├── profiler.py
├── timing_benchmark.py
├── config/
│   ├── cli_config.yaml
├── src/
│   ├── data/
│   │   ├── lightning_datamodule.py
│   │   └── pytorch_dataset.py
│   └── model/
│       ├── lightning_module.py
│       ├── pytorch_decoder.py
│       ├── pytorch_encoder.py
│       └── pytorch_model.py
├── tests/
│   └── test_lightning_parameters.py
└── output/
</code></pre></div></div> <h3 id="easy-timing-benchmark">easy timing benchmark</h3> <p>This is a tool I made to benchmark the any pipeline. It is designed to work with any PyTorch Lightning module and data module, so you can use it to benchmark your model pipeline. It measures detailed timing information for each step of the training process:</p> <ul> <li>Data loading time</li> <li>Forward pass time</li> <li>Backward pass time</li> <li>Other ops: everything that happens after the backward pass but before the end of the batch. This primarily includes: <ul> <li>The optimizer step (applying gradients to update model weights)</li> <li>Scaler updates (when using mixed precision)</li> <li>Any additional overhead between batches</li> </ul> </li> </ul> <p>I find the timing summary at the end of the script to be very useful. It gives you a good overview of the time spent in each step of the pipeline.</p> <h2 id="usage">Usage</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python timing_benchmark.py <span class="nt">-c</span> config/config.yaml <span class="nt">--epochs</span> 1 <span class="nt">--save-dir</span> results/benchmark/my_model
</code></pre></div></div> <p>Replace the model and data classes with your own.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import your model and data classes here
</span><span class="kn">from</span> <span class="n">src.model.lightning_module</span> <span class="kn">import</span> <span class="n">AutoencoderModule</span> <span class="k">as</span> <span class="n">ModelClass</span>
<span class="kn">from</span> <span class="n">src.data.lightning_datamodule</span> <span class="kn">import</span> <span class="n">DummyDataModule</span> <span class="k">as</span> <span class="n">DataModuleClass</span>
</code></pre></div></div> <h1 id="default-model-and-data-classes-to-use-if-not-overridden-by-command-line-arguments">Default model and data classes to use if not overridden by command line arguments</h1> <p>DEFAULT_MODEL_CLASS = ModelClass DEFAULT_DATA_CLASS = DataModuleClass</p> <h3 id="required-arguments">Required Arguments</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">-c, --config</code>: Path to the YAML configuration file</li> <li> <code class="language-plaintext highlighter-rouge">--model-class</code>: Import path to the model class (e.g., ‘src.model.lightning_module.AutoencoderModule’)</li> <li> <code class="language-plaintext highlighter-rouge">--data-class</code>: Import path to the data module class (e.g., ‘src.data.lightning_datamodule.DummyDataModule’)</li> </ul> <h3 id="optional-arguments">Optional Arguments</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">--epochs</code>: Number of epochs to run (default: 2)</li> <li> <code class="language-plaintext highlighter-rouge">--save-dir</code>: Directory to save results (default: results/benchmark/TIMESTAMP)</li> <li> <code class="language-plaintext highlighter-rouge">--precision</code>: Training precision, choices are “16-mixed”, “16”, “32” (default: “32”)</li> <li> <code class="language-plaintext highlighter-rouge">--device</code>: Device to run on, choices are “cuda”, “cpu” (default: auto-detect)</li> </ul> <h2 id="output">Output</h2> <p>The benchmark tool will create several files in the specified output directory:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">timing_summary.yaml</code>: A YAML file containing detailed timing statistics</li> <li> <code class="language-plaintext highlighter-rouge">timing_results.csv</code>: A CSV file with raw timing data for each batch</li> <li> <code class="language-plaintext highlighter-rouge">batch_time_breakdown.png</code>: A stacked bar chart showing time breakdown per batch</li> <li> <code class="language-plaintext highlighter-rouge">total_batch_time.png</code>: A line plot of total batch time</li> <li> <code class="language-plaintext highlighter-rouge">time_distribution_pie.png</code>: A pie chart of average time distribution</li> <li> <code class="language-plaintext highlighter-rouge">benchmark_config.yaml</code>: A copy of the benchmark configuration</li> </ul> <h2 id="tips-for-optimization">Tips for Optimization</h2> <ol> <li> <p><strong>Data Loading</strong>:</p> <ul> <li>If data loading takes &gt;30% of batch time, check the data pipeline and the dataloader again.</li> </ul> </li> <li> <p><strong>Forward/Backward Pass</strong>:</p> <ul> <li>Try mixed precision training with <code class="language-plaintext highlighter-rouge">--precision 16-mixed</code> for faster computation (discussed in <a href="/blogs/training-at-larger-scale/part6/">5. What Is Next</a>)</li> <li>Consider model architecture changes to reduce computation</li> </ul> </li> <li> <p><strong>Optimizer</strong>:</p> <ul> <li>Experiment with different optimizers and their settings</li> </ul> </li> </ol> <h2 id="analyzing-results">Analyzing Results</h2> <p>The benchmark results will help you identify bottlenecks in your training pipeline:</p> <ul> <li>If <strong>data loading</strong> is the bottleneck, optimize data loading pipeline, increase workers, use caching</li> <li>If <strong>forward pass</strong> is the bottleneck, consider model architecture changes or mixed precision</li> <li>If <strong>backward pass</strong> is the bottleneck, try gradient accumulation or mixed precision</li> <li>If <strong>other ops</strong> is the bottleneck, TODO: check with laurens andy</li> </ul> <h2 id="profiling-check-your-pipeline">Profiling: Check Your Pipeline</h2> <h3 id="what-is-it">What Is It?</h3> <p>Profiling helps you understand where time and resources are spent in your training pipeline. It guides optimization by identifying bottlenecks. The profiler also looks at data part of the pipeline, so it is a good idea to run it after the data part is done.</p> <h3 id="how-does-it-work">How Does It Work?</h3> <p>Look at the provided script to profile your training loop:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>profiler.py
</code></pre></div></div> <p>Import your <code class="language-plaintext highlighter-rouge">dataloader</code> and <code class="language-plaintext highlighter-rouge">model</code> modules, then run the script 3 times with the three profilers:</p> <ul> <li><strong>Simple Profiler</strong></li> <li><strong>Advanced Profiler</strong></li> <li><strong>PyTorch Profiler (Chrome Trace Viewer)</strong></li> </ul> <p>it stores the output in the <code class="language-plaintext highlighter-rouge">output/profiler/{config_name}/profiler_logs</code> folder.</p> <h3 id="interpreting-profiler-outputs--a-quick-guide">Interpreting Profiler Outputs – A Quick Guide</h3> <p>Understanding what the profiler outputs mean is key to optimizing your training pipeline. Here’s what to look for in each profiler and how to make sense of the data.</p> <h3 id="1-fit-simple_profiler_outputtxt--summary-view-simple-profiler">1. <code class="language-plaintext highlighter-rouge">fit-simple_profiler_output.txt</code> – Summary View (Simple Profiler)</h3> <h3 id="what-it-shows">What It Shows:</h3> <ul> <li>High-level summary of function calls</li> <li>Average time per operation</li> <li>Relative contribution of each function to total runtime</li> </ul> <h3 id="how-to-read-it">How to Read It:</h3> <ul> <li>Look at the top-consuming operations — these are usually bottlenecks.</li> <li>Pay attention to data loading functions (<code class="language-plaintext highlighter-rouge">*_dataloader_next</code>, <code class="language-plaintext highlighter-rouge">__next__</code>) — these often take more time than expected.</li> <li>Training loops like <code class="language-plaintext highlighter-rouge">run_training_epoch</code> will typically be a large portion; the key is to ensure they’re not dwarfed by overheads.</li> </ul> <h3 id="when-to-take-action-example">When to Take Action (example):</h3> <ul> <li>If data loading takes a large share of total time (e.g., &gt;40%), your pipeline is I/O-bound.</li> <li>If your model training steps are taking less time than preprocessing, you’re likely under-utilizing the GPU.</li> </ul> <h3 id="2-fit-advanced_profiler_outputtxt--line-level-view">2. <code class="language-plaintext highlighter-rouge">fit-advanced_profiler_output.txt</code> – Line-Level View</h3> <h3 id="what-it-shows-1">What It Shows:</h3> <ul> <li>Function-level granularity (per-call stats)</li> <li>Total calls, total time, average time per call</li> <li>Stack trace to locate the exact code path</li> </ul> <h3 id="how-to-read-it-1">How to Read It:</h3> <ul> <li>Sort by total time and identify high-call-count, low-time ops — these may be optimized or batched.</li> <li>Use stack traces to pinpoint performance sinks inside your own code or framework code.</li> <li>Investigate setup or utility functions being called excessively (e.g., synthetic data generation, logging, checkpointing).</li> </ul> <h3 id="when-to-take-action-example-1">When to Take Action (example):</h3> <ul> <li>If any function is causing a lot of time, (where you expect it to be fast) check if it is necessary.</li> </ul> <h3 id="3-pttracejson--chrome-trace-viewer-pytorch-profiler">3. <code class="language-plaintext highlighter-rouge">pt.trace.json</code> – Chrome Trace Viewer (PyTorch Profiler)</h3> <h3 id="what-it-shows-2">What It Shows:</h3> <ul> <li>Frame-by-frame execution timeline</li> <li>Operator-level breakdown (CPU and GPU)</li> <li>Optional memory usage tracking</li> </ul> <h3 id="how-to-read-it-2">How to Read It:</h3> <ol> <li>Open Chrome and go to <code class="language-plaintext highlighter-rouge">chrome://tracing</code>.</li> <li>Drop in the <code class="language-plaintext highlighter-rouge">.json</code> file.</li> <li>Hover over timeline blocks to see operator names, start/end times, and device usage.</li> </ol> <h3 id="what-to-look-for">What to Look For:</h3> <ul> <li>Long horizontal bars → slow operations (usually backward passes, large convolutions)</li> <li>Gaps between ops → potential I/O waits or CPU/GPU syncs</li> <li>Overlapping CPU/GPU ops → good utilization</li> <li>Memory heatmaps (if enabled) → identify peaks or leaks</li> </ul> <h3 id="when-to-take-action-example-2">When to Take Action (example):</h3> <ul> <li>If any function is causing a lot of time, (where you expect it to be fast) check if it is necessary.</li> <li>If idle gaps exist, investigate DataLoader efficiency</li> </ul> <h2 id="next-steps">Next Steps</h2> <p>Congratulations on optimizing your entire training pipeline! Explore what’s next:</p> <p><a href="/blogs/training-at-larger-scale/part6/">5. What Is Next</a></p> <p>TODO read and review</p> <ul> <li> <a href="https://lightning.ai/docs/pytorch/stable/advanced/compile.html" rel="external nofollow noopener" target="_blank">PyTorch Lightning: <code class="language-plaintext highlighter-rouge">compile</code> for speed</a> + sharding and stuff, not sure if needed RN</li> <li><a href="https://lightning.ai/docs/pytorch/stable/advanced/speed.html" rel="external nofollow noopener" target="_blank">PyTorch Lightning: General speed-up tips</a></li> </ul> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>