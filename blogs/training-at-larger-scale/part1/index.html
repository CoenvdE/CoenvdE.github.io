<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Setup | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 1 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part1/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">The Setup</h1> <p class="post-description">Chapter 1 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Training at larger scale </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part1/"> Ch 1: The Setup </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part2/"> Ch 2: Multi-GPU training </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part3/"> Ch 3: Bigger data in the cloud </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part4/"> Ch 4: Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part5/"> Ch 5: Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> Ch 6: What Is Next </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <h2 id="0-setup">0. Setup</h2> <p>Before diving into optimizations, I will walk you through some best practices and a baseline to give you a solid starting point. Note that my model and pipeline were actually a lot more complicated, but my goal is not how to recreate my complicated pipeline, it is to show you simple examples for you to optimize and improve your own.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0. The Setup/
├── config/
│   └── config.yaml
├── output/
├── src/
│   ├── data/
│   │   ├── __pycache__/
│   │   └── pytorch_dataset.py
│   └── model/
│       ├── __pycache__/
│       ├── pytorch_model.py
│       ├── pytorch_encoder.py
│       └── pytorch_decoder.py
├── tests/
│   ├── __pycache__/
│   ├── __init__.py
│   └── test_parameters.py
├── wandb/
├── pytorch_train.py
└── requirements.txt
</code></pre></div></div> <h3 id="my-model-components-and-overview">My Model (components and overview)</h3> <hr> <p>My model is a (Masked) Autoencoder (with some cool stuff that does not matter for this guide) and has the following key components:</p> <ul> <li><a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">pytorch_encoder</code></a></li> <li><a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">pytorch_decoder</code></a></li> <li> <a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">pytorch_model</code></a> – responsible for the forward pass and loss calculation</li> </ul> <h3 id="my-dataset">My Dataset</h3> <hr> <ul> <li>a <a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">pytorch_dataset</code></a> I use a big, geospatial dataset for my training. For this tutorial, I created a dummy example, but feel free to swap it out for e.g. <a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="external nofollow noopener" target="_blank">CIFAR</a> or any other dataset you like. It is important that this is wrapped into a <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> object.<br> We will come back to data in more detail in <a href="/blogs/training-at-larger-scale/part3/">Chapter 2</a>.</li> </ul> <h3 id="reproducibility">Reproducibility</h3> <hr> <p>Reproducibility is very important! It allows you and others to reliably verify and compare results. In research, it ensures findings are valid and consistent. For practitioners, it makes debugging and iterative experimentation much easier. Seeding is one of the prerequisites for reproducibility. It ensures consistent pseudo random number generation. Different frameworks may use different generators. You need to seed everything (torch, numpy, python, etc.). See the <code class="language-plaintext highlighter-rouge">set_seed()</code> function in <a href="/blogs/training-at-larger-scale/part1/">pytorch_train.py</a> for an example implementation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">13</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Set all seeds for reproducibility.</span><span class="sh">"""</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div></div> <p>Note that I use a <code class="language-plaintext highlighter-rouge">set_seed</code> function to keep everything reproducible in the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file. By default, this seed does <em>not</em> propagate to the Dataloader. This is why I create a <code class="language-plaintext highlighter-rouge">generator</code> for the data loader that has a set seed, for which I use the seed from the <code class="language-plaintext highlighter-rouge">set_seed</code> function. Pytorch dataloader workers need to be seeded, because each worker runs in its own process. Without explicit seeding, they will use random seeds, leading to <strong>non-deterministic data loading and augmentations</strong>. Look at the <code class="language-plaintext highlighter-rouge">worker_init_fn</code> in the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file for an example.</p> <p>For <strong>reproducibility during debugging and testing</strong>, set the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># Default: False
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="bp">False</span>     <span class="c1"># Default: True
</span></code></pre></div></div> <p>These flags control CUDA kernel selection and execution:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">deterministic = True</code>: Ensures consistent results across training runs, though this may impact training speed</li> <li> <code class="language-plaintext highlighter-rouge">benchmark = False</code>: Disables CuDNN’s auto-tuner that normally selects optimal convolution algorithms. While auto-tuning can improve performance, it may introduce non-deterministic behavior, particularly with varying input sizes</li> </ul> <p>When both flags are set, you get consistent, reproducible behavior across all training runs.</p> <h3 id="config-files">Config files</h3> <hr> <p>I use config files for the model, dataloader, training, optimizer, and scheduler arguments. This is best practice, allowing quick adjustments to hyperparameters without modifying the code. This makes experimentation and testing more efficient.<br> An example of a config file with a few of these parameters is given in this <a href="/blogs/training-at-larger-scale/part1/">config.yaml</a></p> <h3 id="unittests">Unittests</h3> <hr> <p>Before moving forward, ensure your model actually works. It is important to write tests that check:</p> <ul> <li>Proper parameter loading from config file</li> <li>Shape consistency</li> <li>Correct device allocation</li> </ul> <p>This first one is very important and can save you a lot of trouble debugging, I made an example test for this in my <a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">.tests</code> folder</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> unittest tests/test_parameters.py
</code></pre></div></div> <p>Additionally, I set up automated testing — every push to the GitHub branch runs these tests.<br> For this, look at the <a href="/blogs/training-at-larger-scale/part1/"><code class="language-plaintext highlighter-rouge">.github/workflows/test.yml</code></a> file. While unit tests don’t catch everything, they help ensure the code runs smoothly (in the cloud) and prevent accidental breakage. Automatically running them via GitHub gives you good feedback on if you break anything! This can potentially save you a lot of time and money, since it reduces the likelihood of a situation where you spin up a large GPU cluster for model training and then have to spend an hour or more fixing bugs that you failed to catch before.</p> <h3 id="tracking--experiment-logging-wandb">Tracking &amp; Experiment Logging (WandB)</h3> <hr> <p>Proper tracking is essential for monitoring model performance and debugging issues.<br> I use Weights &amp; Biases (WandB), but alternatives like MLflow also work.</p> <p>What to track?</p> <ul> <li>Validation loss &amp; training loss</li> <li>Learning rate</li> <li>Reconstructions &amp; visualizations to monitor model progress</li> <li>Config files &amp; training logs for reproducibility</li> </ul> <p>You can decide for yourself what you want to track and when (per step, per epoch), it never hurts to track more things!<br> Give the training loop a go to see what happens:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python pytorch_train.py
</code></pre></div></div> <h3 id="validating-your-model-architecture">Validating Your Model Architecture</h3> <hr> <p>Validating your model architecture early prevents costly debugging later in training—especially when working with large or domain-specific datasets. These problems often involve unique data formats and multi-dimensional inputs that don’t fit a model architecture out of the box. Skipping this step can lead to wasted time and compute on models that silently fail to learn. These validation steps are unique for each problem, but the general idea is to validate the model on a smaller dataset that are representative of the full dataset.</p> <p>Example: the problem I need to solve is a geospatial one, meaning my model had to handle multiple dimensions (channels, height, width, time). Before experimenting with my full geospatial dataset, I verified that the model could train properly on local hardware (and a single GPU). I validated the architecture using the following datasets:</p> <ul> <li> <strong>Dummy Dataset</strong>: Randomly initialized tensors with the correct shape. This confirmed the model accepted input as expected and helped verify the pipeline was structurally sound.</li> <li> <strong>CIFAR-10</strong>: Although not geospatial, CIFAR-10 contains spatial dimensions (height and width), 3 variable channels (RGB), and can simulate a time dimension (e.g. <code class="language-plaintext highlighter-rouge">time=1</code>). This made it a useful proxy for early validation.</li> <li> <strong>Subset of the geospatial dataset</strong>: A small slice of the real dataset ensured the model worked with actual data formats and domain-specific characteristics.</li> </ul> <p>This step ensured my architecture was robust, flexible, and ready for large-scale training.</p> <hr> <h3 id="environmental-impact-monitoring">Environmental Impact Monitoring</h3> <p>AI models consume significant computational resources and energy. Monitoring environmental impact helps quantify your carbon footprint, which is important for sustainability and responsible AI development. For this reason I made a <a href="/blogs/training-at-larger-scale/part1/">monitoring script</a> to monitor the emissions of any training run. You do not need to modify anything in your training script, just run my script with your training command as shown below. My script is based on the <a href="https://github.com/mlco2/codecarbon" rel="external nofollow noopener" target="_blank">codecarbon</a> package. You may need to fill in the password of your machine to give codecarbon access to monitoring your hardware.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run your training with emissions monitoring</span>
python monitor_training.py <span class="s2">"python pytorch_train.py"</span>
</code></pre></div></div> <p>For each run, the script creates a dedicated folder with:</p> <ul> <li>A timestamped log file capturing all training output</li> <li>Carbon emissions data in CSV format (detailed breakdown of emissions by component)</li> <li>Visualizations showing emissions and energy breakdown by component (CPU/GPU/RAM)</li> <li>A summary report with key metrics</li> </ul> <hr> <p>Now that the model is working, validated and tracking was set up, it is time to dive into how to make it efficient and suitable for <a href="/blogs/training-at-larger-scale/part2/">larger scale training with multiple GPUs</a>. Before proceeding to the actual training, it’s crucial to set up proper fail safes. Training for multiple days without these safeguards can lead to catastrophic failures, wasted resources, and lost progress. Next to the ones we already set up, I’ll cover some extra ones in the next chapters.</p> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>