<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Optimizing the pipeline: Data | Coen </title> <meta name="author" content="Coen van den Elsen"> <meta name="description" content="Chapter 4 of the Training at Larger Scale series"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://coenvde.github.io/blogs/training-at-larger-scale/part4/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Coen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blogs/">Blogs <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Github </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="toggle-nav-container"> <button class="toggle-nav" aria-label="Toggle navigation"> <span class="toggle-nav-icon"></span> <span class="toggle-nav-text">Toggle navigation</span> </button> </div> <h1 class="post-title">Optimizing the pipeline: Data</h1> <p class="post-description">Chapter 4 of the Training at Larger Scale series</p> </header> <div class="row"> <div class="col-sm-12 col-md-3 blog-sidebar"> <div class="sidebar-sticky"> <div class="collection-link"> <a href="/blogs/" class="back-to-collection"> <i class="fas fa-arrow-left"></i> Back to Collections </a> </div> <h3>Contents</h3> <div class="toc-container"> <ul class="blog-nav-list"> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/index/"> Training at larger scale </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part1/"> 1. The Setup </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part2/"> 2. Multi-GPU training </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part3/"> 3. Bigger data in the cloud </a> </li> <li class="blog-nav-item active"> <a href="/blogs/training-at-larger-scale/part4/"> 4. Optimizing the pipeline: Data </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part5/"> 5. Optimizing the pipeline: Model </a> </li> <li class="blog-nav-item"> <a href="/blogs/training-at-larger-scale/part6/"> 6. What Is Next </a> </li> </ul> </div> <style>.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.toc-container{padding:.5rem 0}.blog-nav-item a{display:block;text-decoration:none;color:var(--global-text-color)}.blog-nav-item:hover a{text-decoration:none}.blog-nav-item.active a{text-decoration:none}</style> </div> </div> <div class="col-sm-12 col-md-9 blog-content"> <article> <h2 id="3-optimizing-the-pipeline-data">3. Optimizing the pipeline: Data</h2> <p>Efficient data loading can significantly reduce training time — especially when the GPU is fast but starved for data.</p> <ul> <li>The GPU (or TPU/HPU etc.) is <strong>the most expensive and performance-critical component</strong> in a training pipeline.</li> <li>If the data pipeline (i.e., loading, preprocessing, transferring) is slow, the GPU will sit idle, waiting for the next batch.</li> <li>The goal is to <strong>saturate the GPU</strong> — keep it busy with minimal idle time.</li> </ul> <h3 id="what-you-want">What You Want</h3> <ul> <li>A data pipeline <strong>that saturates the GPU as much as possible</strong> without introducing too much memory pressure or over demanding CPUs.</li> <li>Each DataLoader worker is effectively a producer that loads data in parallel, while your training loop (running on the main process/GPU) is the consumer. The goal is to maximize overlap: while one batch is being processed on the GPU, the next batch (or several batches) are being loaded by the CPU workers in the background.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3. Optimizing the pipeline: Data/
├── benchmark_datapipeline_configurations.py  # Main benchmark script
├── plot_benchmark_runs.ipynb                # Analysis notebook
├── dummydataset.py                          # Dummy dataset for testing
├── utils.py                                 # Utility functions
├── benchmark_logs/                          # Benchmark results
└── benchmark_imgs/                          # Visualizations
</code></pre></div></div> <h3 id="key-concepts-and-metrics">Key Concepts and Metrics</h3> <hr> <ul> <li> <p><strong>CPU (cores)</strong>:</p> <ul> <li>Used by the <strong>main process and workers</strong> to (lazy) load, transform, and prepare data for training.</li> <li>Ideally, you want high CPU usage (to indicate workers are busy) but not so high that the OS or other processes starve. If your CPU utilization is peaking at 100% on all cores and the system becomes unresponsive or the throughput plateaus, you may have too many workers or threads.</li> <li>You can check available CPUs using:</li> </ul> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="nf">print</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">())</span>
</code></pre></div> </div> </li> <li> <p><strong>RAM (System Memory / CPU RAM)</strong>:</p> <ul> <li>Used by workers to load batches into memory.</li> <li>Important when working with large datasets, large batch sizes, or complex transforms.</li> <li>Too many workers without enough RAM can lead to crashes or swapping, which hurts performance.</li> </ul> </li> <li> <p><strong>GPU Saturation</strong>:</p> <ul> <li>The key performance target.</li> <li>If GPU is underutilized, bottleneck is likely in data loading.</li> </ul> </li> <li> <p><strong>I/O Considerations</strong></p> <ul> <li> <p><strong>Bandwidth</strong>: The maximum rate of data transfer between components in your system:</p> <ul> <li>Disk/cloud storage to RAM</li> <li>CPU memory to GPU memory</li> <li>Network transfers (e.g., from cloud storage to your instance)</li> </ul> <p>Measured in MB/s or GB/s. High bandwidth allows more data to flow per second, while low bandwidth creates bottlenecks that can leave your GPU waiting for data. When working with cloud data, network bandwidth often becomes the primary constraint. Cloud servers typically have significantly higher bandwidth (often 10-100x faster) than consumer (wifi) internet connections, which can dramatically improve data transfer speeds when training in cloud environments compared to local setups.</p> </li> <li> <strong>Chunking</strong>: Use optimal chunk sizes to balance I/O overhead and memory usage</li> <li> <strong>Optimizations</strong>: <ul> <li>Memory mapping for large files</li> <li>Prefetching data to reduce latency</li> <li>Caching frequently used data</li> <li>Sequential access over random reads</li> <li>Compression to reduce I/O</li> </ul> </li> <li> <strong>File Formats</strong>: Choose ML-optimized formats (Parquet, TFRecord, WebDataset, Zarr)</li> </ul> </li> </ul> <p><strong>Streaming Workers (Dask) vs DataLoader Workers (PyTorch)</strong></p> <p>PyTorch DataLoader and streaming frameworks like Dask can be combined effectively to stream cloud data into your training pipeline, but it’s crucial to understand they operate at different layers of the data process:</p> <ul> <li> <p><strong>Dask (or other streaming frameworks)</strong> handles the low-level data access by:</p> <ul> <li>Building a computational graph (DAG) for lazy evaluation</li> <li>Managing how data chunks are read from storage</li> <li>Orchestrating parallel fetching of data from cloud/disk</li> <li>Optimizing memory usage through chunking strategies</li> </ul> </li> <li> <p><strong>PyTorch DataLoader</strong> manages the training-specific data handling by:</p> <ul> <li>Creating worker processes to parallelize sample preparation</li> <li>Implementing batching, shuffling, and sampling strategies</li> <li>Executing the preprocessing defined in your <code class="language-plaintext highlighter-rouge">__getitem__</code> method</li> <li>Transferring prepared batches to the GPU</li> </ul> </li> </ul> <p>These systems don’t automatically coordinate with each other. The connection point occurs when a DataLoader worker calls <code class="language-plaintext highlighter-rouge">__getitem__</code> on your Dataset, which then triggers Dask to materialize the required data chunks when calling a <code class="language-plaintext highlighter-rouge">.load()</code> function. By understanding this separation, you can optimize each layer independently:</p> <ol> <li>Configure optimal data reading (e.g. chunk sizes, thread count, worker count)</li> <li>Configure DataLoader for optimal batch preparation (e.g. num_workers, prefetch_factor)</li> </ol> <p>It’s important to avoid conflicting parallelism between these systems because too many concurrent processes and threads can lead to resource contention and degraded performance.</p> <h3 id="practical-guidelines-background-and-setup">Practical guidelines, background and setup</h3> <hr> <p>I have created scripts to help you optimize your data pipeline, but before we dive into the benchmarking and optimization, it is important to understand how everything works.</p> <p><strong>Understanding DataLoader Workers and Process Distribution</strong></p> <p>When optimizing your data pipeline, it’s important to understand how worker processes are distributed across your available CPU cores and to leave sufficient CPU resources for main processes and system overhead.</p> <p>Reserve 1-2 CPU cores per GPU for the main training process and system overhead. When using containerization solutions like Docker, you may need to reserve additional CPU resources for container management.</p> <ul> <li> <p><strong>Single GPU setup</strong>: With a single GPU and one main training process, you’ll have a single DataLoader that spawns <code class="language-plaintext highlighter-rouge">num_workers</code> processes. For example, on a machine with 20 CPU cores and 1 GPU, you should reserve 1-2 cores for the main process and system overhead, leaving about 18 cores for DataLoader workers.</p> </li> <li> <p><strong>Multi-GPU setup</strong>: With multiple GPUs (e.g., using DistributedDataParallel), each GPU has its own main process, and each main process creates its own set of DataLoader workers. For instance, on a machine with 8 GPUs and 160 CPU cores, you should reserve 8-16 cores for main processes and system overhead (1-2 per GPU), leaving approximately 144-152 cores to be distributed among all worker processes.</p> </li> </ul> <p>To avoid CPU contention, ensure the total number of worker processes doesn’t exceed your available CPU cores minus the cores needed for main processes and system overhead. When workers compete for CPU resources, data loading becomes inefficient and can slow down training.</p> <p><strong>CPU allocation examples</strong>:</p> <ul> <li>20 CPUs, 1 GPU: Reserve 2 CPUs for main process and overhead, use up to 18 workers</li> <li>60 CPUs, 3 GPUs: Reserve 6 CPUs (2 per GPU) for main processes and overhead, use up to 18 workers per GPU (54 total)</li> </ul> <p>Again, note that num_workers is not about CPU’s but processes, and a process may use more or less than 1 cpu core.</p> <p><strong>Set batch size</strong> The ideal batch size depends on your dataset characteristics, model complexity, and available GPU memory. You want a size that’s stable but still provides stochastic gradient descent benefits. For my use case, 32 worked well, but you may need to adjust based on your specific requirements. This batch size is needed because it is used in my optimization benchmark script later.</p> <p><strong>Optional: measure time for 1 batch to train</strong> Measuring the time it takes to (load a mini batch and) complete a single training step. This information is useful for properly configuring the benchmark script parameters to accurately reflect your real-world training conditions. For this, you can run the <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/timing_benchmark.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">timing_benchmark.py</code></a> script that is in the folder of <a href="/blogs/training-at-larger-scale/part5/">chapter 4</a>.</p> <p><strong>No need to over-optimize the DataLoader</strong>: If your model is small or the GPU is not very powerful, there’s no point in using 16+ workers or heavy parallel jobs when the GPU is already saturated. I have spent quite some time doing research on this, and it is important to think about this step as it can drastically increase your performance, but there is no need to do “grid search” such-like stuff for this. Follow my guide and your speed should already improve a lot. Experimenting endlessly with this also costs money and potential experimentation time. I’ll tell you more on how to do it in a sec.</p> <h3 id="when-optimizing-data-loading-identify-the-bottleneck-using-the-benchmark-script-i-created-look-for-these-key-indicators">When optimizing data loading, identify the bottleneck using the <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/benchmark_configurations.py" rel="external nofollow noopener" target="_blank">benchmark script</a> I created. Look for these key indicators:</h3> <hr> <p>• <strong>GPU Under-utilization</strong>: If your GPU is frequently idle, your data pipeline isn’t keeping up. The benchmark will show long waits between training steps and white spaces in the “train row” of the visualization.</p> <p>• <strong>Single CPU core maxed out</strong>: If one CPU core is at 100% while others are idle (especially with num_workers=0), increase num_workers to distribute the load across multiple cores.</p> <p>• <strong>All CPU cores busy but GPU still waiting</strong>: This suggests an I/O bottleneck (slow disk, network, etc.). Adding more workers won’t help. Instead, optimize at the dataset level with faster storage, caching, or better chunking strategies.</p> <h3 id="what-can-be-optimized-dataset">what can be optimized: <strong><em>dataset</em></strong> </h3> <hr> <p>This part is not always needed. If there are no parameters, chunking or file format to be optimized, focus on the dataloader instead. (when streaming from machine’s disk memory, ssd or when streaming is all handled automatically). What to optimize:</p> <ol> <li> <strong>Efficient Storage Formats</strong>:</li> </ol> <p>Initially, my data was stored in NetCDF files, which are common for scientific data but can be inefficient when working with streaming from cloud storage. The default chunking in these files was not optimized for machine learning, causing unnecessary data to be loaded into memory during training. To address this, I stored everything in <a href="https://zarr.readthedocs.io/" rel="external nofollow noopener" target="_blank">Zarr</a>. Zarr is specifically designed for fast cloud-based data access. I will not include the migration in this blog, as it is out of scope, I just want to show that it is important to think about the data formats used.</p> <ol> <li> <strong>Optimal Chunking</strong>:</li> </ol> <p><strong><em>How to calculate (handwavy) how big your chunks should be</em></strong></p> <p>NOTE SKIP THIS FOR FEEDBACK: properly write later, but basically chunks small enough so that in worst case scenario, the time it loads the least chunks possible and best case it loads exactly one batch of data (or something like this)</p> <ol> <li><strong>Parallelize I/O: Loading Efficiently</strong></li> </ol> <p>TO READER: DOES IT GO IN TO MUCH DETAIL AS WE USE ZARR FOR THIS IN THE END? IS IT USEFUL FOR READERS?</p> <p>Some libraries, like Dask, can parallelize reading within a dataset, providing their own optimization parameters. However, be careful when combining these with PyTorch DataLoader workers, as this can lead to resource contention and diminishing returns.</p> <p>When working with lazy-loading/streaming data into GPU memory, there are several parameters you might need to optimize:</p> <ol> <li> <p><strong>Parallel Loading Parameters</strong>:</p> <ul> <li>Number of concurrent readers/workers (processes that execute computations)</li> <li>Memory limits per worker (to prevent OOM errors)</li> <li>Thread pool size per worker (for parallel execution within a worker)</li> </ul> </li> <li> <p><strong>Caching Parameters</strong>:</p> <ul> <li>Cache size limits (to prevent OOM errors)</li> <li>Cache eviction policies (to prevent OOM errors)</li> <li>Persistent vs. in-memory caching (to prevent OOM errors)</li> </ul> </li> </ol> <p>I will explain how to optimize these parameters in a little bit.</p> <p><strong>Usecase: Understanding Dask Execution Modes</strong></p> <p>Dask offers three execution modes, each with different trade-offs:</p> <ol> <li> <p><strong>Single-Threaded Scheduler</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">dask.config.set(scheduler="single-threaded")</code> </li> <li>All tasks run sequentially in the main thread</li> <li>No parallelism but minimal overhead</li> <li>Useful for debugging or when relying entirely on DataLoader’s parallelism</li> <li>In multi-GPU setups, each process runs its own sequential scheduler</li> </ul> </li> <li> <p><strong>Threaded Scheduler</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">dask.config.set(scheduler="threads", num_workers=dask_threads)</code> </li> <li>Tasks run in parallel using a thread pool within a single process</li> <li>Good for I/O-bound operations (like reading data chunks)</li> <li>Moderate parallelism with low overhead</li> <li>In multi-GPU setups, be careful of the total thread count (e.g., 8 processes × 4 threads = 32 threads)</li> </ul> </li> <li> <p><strong>Distributed Cluster</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">LocalCluster</code> and <code class="language-plaintext highlighter-rouge">Client</code> to create a full Dask cluster</li> <li>Runs multiple worker processes, each with multiple threads</li> <li>Provides process-level parallelism, bypassing Python’s GIL</li> <li>Includes a dashboard for monitoring tasks and resource usage</li> <li>Options for per-GPU-process clusters or a single shared cluster</li> <li>Higher overhead but better isolation and monitoring capabilities</li> </ul> </li> </ol> <p>For a single GPU with limited CPUs (e.g., 20 cores):</p> <ul> <li>A threaded scheduler with 4-8 threads is often sufficient</li> <li>A small distributed cluster (1-2 workers, 4 threads each) offers better monitoring</li> </ul> <p>For multi-GPU setups (e.g., 8 GPUs with 20 cores):</p> <ul> <li>Be careful not to oversubscribe your CPU resources</li> <li>If each GPU process uses its own Dask cluster, limit to 1 worker with 2 threads per process</li> <li>Consider using a single shared Dask cluster for all GPU processes</li> <li>Monitor CPU utilization to avoid contention</li> </ul> <p>The key is balancing parallelism against resource constraints. More parallelism isn’t always better, especially when resources are shared across multiple GPU processes. Start conservative and scale up while monitoring performance.</p> <ol> <li> <strong>Caching and Locality</strong>: For remote data, implement caching strategies to avoid repeatedly downloading the same data. Consistent access patterns help leverage OS-level caching.</li> </ol> <p>If your data pipeline doesn’t use these specialized libraries, you can focus solely on DataLoader optimization.</p> <p>In summary, dataset-level optimization is about making data access as efficient as possible. By storing data smartly (chunked, compressed appropriately, possibly colocated with training if remote), and by only doing minimal necessary work for each access, you ensure that the raw data supply is fast. Once that is in place, DataLoader-level tuning can further amplify the throughput.</p> <p>For more insights on optimizing cloud data loading, see <a href="https://earthmover.io/blog/cloud-native-dataloader/" rel="external nofollow noopener" target="_blank">Earthmover’s guide to cloud-native dataloaders</a> covering streaming techniques, I/O optimization, and resource balancing.</p> <h3 id="what-can-be-optimized-dataloader">what can be optimized: <strong><em>dataloader</em></strong> </h3> <hr> <p>The DataLoader is critical for training performance. Key parameters to optimize:</p> <ol> <li> <p><strong><code class="language-plaintext highlighter-rouge">num_workers</code></strong>: Controls parallel data loading subprocesses</p> <ul> <li>Too few: GPU waits for data</li> <li>Too many: Resource contention, diminishing returns</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">prefetch_factor</code></strong>: Batches loaded in advance per worker</p> <ul> <li>Default is 2, which works for most cases</li> <li>Adjust based on sample size and memory requirements</li> <li>Total prefetched = <code class="language-plaintext highlighter-rouge">num_workers * prefetch_factor</code> </li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">pin_memory</code></strong>: Enables faster CPU to GPU transfers</p> <ul> <li>Set to <code class="language-plaintext highlighter-rouge">True</code> when using GPU</li> <li>Creates page-locked memory for direct transfers</li> <li>Slightly increases CPU memory usage</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">persistent_workers</code></strong>: Keeps workers alive between epochs</p> <ul> <li>Reduces worker initialization overhead</li> <li>Useful for large datasets and complex initialization</li> <li>Significantly reduces epoch transition time</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">multiprocessing_context</code></strong>: Controls worker process spawning</p> <ul> <li>Options: ‘fork’, ‘spawn’, ‘forkserver’</li> <li>Use ‘forkserver’ (or ‘spawn’) for better CUDA compatibility</li> <li>Note: for more detail, you can read the pytorch docs <a href="https://docs.pytorch.org/docs/stable/notes/multiprocessing.html" rel="external nofollow noopener" target="_blank">here</a> </li> </ul> </li> </ol> <p>Even with an efficient dataset, proper DataLoader settings are crucial.</p> <h3 id="optimizing-benchmarking-tools-for-dataloader-optimization">Optimizing: Benchmarking Tools for DataLoader Optimization</h3> <p>I created two files to help you optimize your data pipeline:</p> <ol> <li> <p><strong><a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/benchmark_configurations.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code></a></strong></p> <ul> <li>suitable for any pytorch Dataset object</li> <li>Runs and logs different DataLoader configurations</li> <li>Measures performance metrics for each configuration</li> <li>Includes Dask integration (commented out by default for those who don’t need it)</li> <li>Contains a configurable <code class="language-plaintext highlighter-rouge">train_step_time</code> parameter (default: 0.1 seconds) <ul> <li>This simulates model training time</li> <li>You can set this to match your actual model’s training time per batch</li> <li>The goal remains the same: minimize data loading wait times</li> </ul> </li> </ul> </li> <li> <p><strong><a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/plot_benchmark_runs.ipynb" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">plot_benchmark_runs.ipynb</code></a></strong></p> <ul> <li>Jupyter notebook for visualizing benchmark results</li> <li>Creates charts comparing different configurations</li> <li>Helps identify the optimal setup for your specific hardware</li> </ul> </li> </ol> <p>If you’re using Dask, you can also leverage the Dask dashboard to monitor:</p> <ul> <li>Worker memory usage</li> <li>CPU utilization</li> <li>Task execution</li> <li>Resource bottlenecks</li> </ul> <h3 id="example-benchmark-results">Example Benchmark Results</h3> <p>Below are example benchmark results showing the dramatic difference between an unoptimized baseline configuration and an optimized one:</p> <div align="center"> <img src="/images/training-blog/benchmark_logs_bad_baseline.png" alt="Unoptimized Baseline" width="400"> </div> <div align="center"> <img src="/images/training-blog/benchmark_logs_good_baseline.png" alt="Optimized Configuration" width="400"> <p><em>Unoptimized baseline vs optimized configuration performance after 3 epochs with 50 batches</em></p> </div> <div align="right"> <small>Source: <a href="https://blog.dailydoseofds.com/p/4-strategies-for-multi-gpu-training" rel="external nofollow noopener" target="_blank">Daily Dose of Data Science</a></small> </div> <p>With these simple optimizations, my dataloader became 5 times faster than the baseline. The people that made the first version of this benchmark script was able to run 15x faster. This performance improvement is transformative for training - where others might only complete 50 epochs in a given timeframe, I was able to run 250 epochs. This acceleration dramatically reduces overall training time and allows for more experimentation and model iterations.</p> <h3 id="practical-optimization-approach">Practical Optimization Approach</h3> <hr> <ol> <li><strong>Import your own dataset at the start</strong></li> </ol> <ul> <li>Follow these steps to systematically optimize your data pipeline: change the line on top of <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/benchmark_configurations.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code></a>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dummydataset</span> <span class="kn">import</span> <span class="n">DummyDataset</span> <span class="k">as</span> <span class="n">YourDataset</span> <span class="c1"># replace with your dataset
</span></code></pre></div></div> <ol> <li> <p><strong>Establish a baseline</strong></p> <ul> <li>Start with minimal configuration: <ul> <li> <code class="language-plaintext highlighter-rouge">num_workers = 0</code> (single-process loading)</li> <li> <code class="language-plaintext highlighter-rouge">prefetch_factor = None</code> (default behavior)</li> <li><code class="language-plaintext highlighter-rouge">persistent_workers = False</code></li> <li><code class="language-plaintext highlighter-rouge">pin_memory = False</code></li> </ul> </li> <li>This provides a reference point for measuring improvements</li> </ul> </li> <li> <p><strong>Implement a sensible default configuration</strong></p> <ul> <li>For a system with 10 CPU cores and 1 GPU: <ul> <li>Reserve 1-2 cores for the main training process</li> <li>Allocate remaining cores between DataLoader workers and streaming processes</li> <li>Example allocation: <ul> <li>5 cores for Dask streaming workers (if using Dask)</li> <li>8 cores for DataLoader workers</li> </ul> </li> <li>Note: Some CPU oversubscription (e.g., 13 workers on 10 cores) can be beneficial <ul> <li>When workers are waiting for I/O operations, they don’t use CPU</li> <li>This can increase overall throughput by reducing idle time</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><strong>Experiment with different configurations</strong></p> <ul> <li>Test variations systematically: <ul> <li>More streaming workers, fewer DataLoader workers</li> <li>More DataLoader workers, fewer streaming workers</li> <li>Higher CPU oversubscription</li> <li>Optional: Even higher oversubscription</li> <li>Lower CPU oversubscription</li> <li>Different <code class="language-plaintext highlighter-rouge">prefetch_factor</code> values</li> </ul> </li> </ul> </li> <li> <p><strong>Run benchmarks and analyze results</strong></p> <ul> <li>Use <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/benchmark_configurations.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code></a> to test all configurations</li> <li>Can be run locally or in the cloud</li> <li>(Download and) analyze the files with <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/plot_benchmark_runs.ipynb" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">plot_benchmark_runs.ipynb</code></a> (use the <code class="language-plaintext highlighter-rouge">.log</code> files, not the <code class="language-plaintext highlighter-rouge">_workers.log</code> files)</li> <li>Compare performance metrics across configurations</li> </ul> </li> </ol> <p>Use the <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/3.%20Optimizing%20the%20pipeline%3A%20Data/plot_benchmark_runs.ipynb" rel="external nofollow noopener" target="_blank">plotting notebook</a> to visualize differences between runs and identify which configuration has the lowest wait/batch fetching time. While I don’t explicitly measure vRAM/CPU utilization in these tools (as it’s complex and time-consuming to implement), the primary goal is to significantly improve training time with reasonable effort. Having that said, watch for these warning signs:</p> <ul> <li> <p><strong>Memory issues</strong>: If memory usage spikes, reduce workers, prefetch factor, or caching.</p> </li> <li> <p><strong>Diminishing returns</strong>: If adding workers doesn’t improve throughput, you’ve reached saturation.</p> </li> <li> <p><strong>Memory leaks</strong>: If RAM usage continuously grows, check for Dask worker caching, large objects in persistent workers, or reference issues.</p> </li> </ul> <h3 id="cloud-vs-local-performance">Cloud vs. Local Performance</h3> <p>Note that network bandwidth varies dramatically between environments. When moving from local development (WiFi) to cloud training, you may see orders of magnitude improvement in data loading speed. In my case, I observed a 100x decrease in wait time when moving to the cloud. Always benchmark in the same environment where you’ll be training, as the optimal configuration can differ significantly between local and cloud setups.</p> <p>Now that we have the data-part of the pipeline optimized, lets focus on the <a href="https://github.com/CoenvdE/Training-at-larger-scale-blog/blob/main/4.%20Optimizing%20the%20pipeline%3A%20Model.md" rel="external nofollow noopener" target="_blank">Model</a></p> </article> </div> </div> </div> <style>.blog-sidebar{border-right:1px solid var(--global-divider-color);padding-right:20px}.sidebar-sticky{position:sticky;top:4rem;height:calc(100vh - 6rem);overflow-y:auto}.blog-nav-list{list-style-type:none;padding-left:0;margin-top:15px}.blog-nav-item{margin-bottom:10px;padding:5px 10px;border-radius:4px;transition:background-color .3s ease}.blog-nav-item:hover{background-color:var(--global-hover-color)}.blog-nav-item.active{background-color:var(--global-theme-color);font-weight:bold}.blog-nav-item.active a{color:white}.collection-link{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid var(--global-divider-color)}.back-to-collection{display:block;padding:8px 10px;background-color:var(--global-code-bg-color);border-radius:4px;text-decoration:none;transition:background-color .3s ease}.back-to-collection:hover{background-color:var(--global-hover-color);text-decoration:none}.chapters-container{display:flex;flex-direction:column;gap:20px;margin:30px 0}.chapter-card{padding:20px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,0.1);transition:all .3s ease;position:relative;background-color:var(--global-card-bg-color)}.chapter-card:hover{box-shadow:0 5px 15px rgba(0,0,0,0.1);transform:translateY(-2px)}.chapter-number{font-weight:bold;color:var(--global-text-color-light);margin-bottom:5px}.chapter-title{margin-top:0;margin-bottom:10px;color:var(--global-theme-color)}.chapter-description{margin-bottom:15px;color:var(--global-text-color)}.toggle-nav-container{text-align:right;margin-bottom:1rem}.toggle-nav{display:inline-block;background:transparent;border:0;cursor:pointer;padding:.5rem;font-size:.9rem;color:var(--global-text-color)}.toggle-nav-icon{display:inline-block;width:1.25rem;height:2px;background-color:var(--global-text-color);position:relative;margin-right:.5rem;vertical-align:middle}.toggle-nav-icon:before,.toggle-nav-icon:after{content:'';display:block;width:100%;height:2px;background-color:var(--global-text-color);position:absolute;left:0}.toggle-nav-icon:before{top:-6px}.toggle-nav-icon:after{bottom:-6px}.toggle-nav-text{vertical-align:middle}@media(max-width:768px){.blog-sidebar{border-right:0;border-bottom:1px solid var(--global-divider-color);margin-bottom:20px;padding-bottom:20px}.sidebar-sticky{position:relative;height:auto;overflow-y:visible}.blog-nav-list{display:flex;flex-wrap:wrap;gap:10px}.blog-nav-item{margin-bottom:0}}</style> <script>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleBtn = document.querySelector('.toggle-nav');
    const sidebar = document.querySelector('.blog-sidebar');
    const content = document.querySelector('.blog-content');

    if (toggleBtn && sidebar && content) {
      toggleBtn.addEventListener('click', function () {
        if (sidebar.style.display === 'none') {
          sidebar.style.display = 'block';
          content.classList.remove('col-md-12');
          content.classList.add('col-md-9');
        } else {
          sidebar.style.display = 'none';
          content.classList.remove('col-md-9');
          content.classList.add('col-md-12');
        }
      });
    }
  });
</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Coen van den Elsen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>