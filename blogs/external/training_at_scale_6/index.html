<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>layout: external_blog title: “Cloud storage credentials\nCLOUD_STORAGE_KEY=your_secret_key_here\nCLOUD_STORAGE_BUCKET=training-data-bucket\n\n# Experiment tracking\nWANDB_API_KEY=your_wandb_key_here\nEXPERIMENT_NAME=transformer_v2\n```\n\n<strong>Important</strong>: Never commit your <code class="language-plaintext highlighter-rouge">.env</code>file to your repository. It often contains sensitive information like API keys and credentials. Instead:\n\n1. Add<code class="language-plaintext highlighter-rouge">.env</code>to your<code class="language-plaintext highlighter-rouge">.gitignore</code>file\n2. Provide a<code class="language-plaintext highlighter-rouge">.env.example</code> template with dummy values in the repo\n3. Document the required environment variables in your README\n4. For cloud deployments, use the platform’s secrets management (AWS Secrets Manager, GitHub Secrets, etc.)\n\nThis approach keeps your secrets secure while making configuration straightforward for team members.\n\n—\n\n## Pre-training Strategy: Experiment First\n\nBefore jumping into full-scale training:\n\n- Select a baseline configuration \n- Observe the loss curves over epochs \n- Run small-scale experiments to test different settings \ \n- Compare outcomes to identify the best setup \n\nThis approach helps avoid wasting compute and accelerates convergence.\n\nTODO: add a picture of the loss curves and explain some examples\n\n\n## What’s After This Guide?\n\n### Faster\n\n- Look into <strong>JAX</strong> for speed and flexibility\n\n### Bigger\n\n- Model parallelism, sharding, and memory optimization techniques like:\n - <strong>Zero Redundancy Optimizer (ZeRO)</strong> \n <a href="https://oracle-oci-ocas.medium.com/zero-redundancy-optimizers-a-method-for-training-machine-learning-models-with-billion-parameter-472e8f4e7a5b" rel="external nofollow noopener" target="_blank">Read more here</a>\n\nThese are advanced topics — no need to rush. Focus on working with the current tools first.\n” description: These are advanced topics — no need to rush. Focus on working with the current tools first. date: ‘2025-05-05’ collection: external_blogs collection_id: training_at_scale chapter_number: 6 source_repo: https://github.com/CoenvdE/Training-at-larger-scale-blog original_file: “/5. What Is Next.md”</p> <hr> <h2 id="5-whats-next">5. What’s Next?</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>5. What Is Next/
├── cli_config.yaml
</code></pre></div></div> <h3 id="docker--and-why-its-nice">Docker — And Why It’s Nice</h3> <p>Docker lets you <strong>containerize</strong> your environment, making sure your code runs the same everywhere—on your laptop or in the cloud.<br> Why it’s nice:</p> <ul> <li>Reproducible setups across different machines or cloud providers (Digital Ocean, AWS, etc.)</li> <li>Lightweight &amp; portable</li> <li>Easy to test in isolated environments</li> </ul> <hr> <h3 id="precision-in-training-in-pytorch-lightning">Precision in Training (in PyTorch Lightning)</h3> <p>TODO: keep here or move to 4. Optimizing the pipeline: Model.md?</p> <p>Choosing the right precision boosts training <strong>speed</strong> and <strong>efficiency</strong> without sacrificing performance—if used correctly. Why it’s nice:</p> <ul> <li>Faster training</li> <li>Bigger batch sizes</li> <li>Less memory usage</li> </ul> <p>see how to add in <code class="language-plaintext highlighter-rouge">config/cli_config.yaml</code></p> <ul> <li> <p><strong><code class="language-plaintext highlighter-rouge">precision="32-true"</code> (default)</strong></p> <ul> <li>Full <code class="language-plaintext highlighter-rouge">float32</code> </li> <li>Most stable</li> <li>Slowest</li> <li>Max memory use</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">precision="16-mixed"</code></strong></p> <ul> <li>Mixed <code class="language-plaintext highlighter-rouge">float16</code> ops, <code class="language-plaintext highlighter-rouge">float32</code> weights</li> <li>Fast, low memory</li> <li>Needs loss scaling</li> <li>Risk of numerical instability</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">precision="bf16-mixed"</code></strong></p> <ul> <li>Mixed <code class="language-plaintext highlighter-rouge">bfloat16</code> ops</li> <li>Fast and stable</li> <li>No loss scaling</li> <li>Requires A100+ or TPU</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">precision="16-true"</code> / <code class="language-plaintext highlighter-rouge">bf16-true</code></strong></p> <ul> <li>Full low precision (float16 or bfloat16)</li> <li>Max speed</li> <li>Risky for training</li> <li>Best for inference/fine-tuning</li> <li>Hardware-dependent</li> </ul> </li> </ul> <h3 id="summary-table">Summary Table</h3> <table> <thead> <tr> <th>Precision</th> <th>Speed</th> <th>Stability</th> <th>Loss Scaling</th> <th>Hardware Requirement</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">32-true</code></td> <td>Slow</td> <td>Highest</td> <td>No</td> <td>None</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">16-mixed</code></td> <td>Fast</td> <td>Medium</td> <td>Yes</td> <td>Most NVIDIA GPUs</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">bf16-mixed</code></td> <td>Fast</td> <td>High</td> <td>No</td> <td>A100+, TPUs</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">16-true</code></td> <td>Fastest</td> <td>Low</td> <td>Risky</td> <td>float16 GPUs</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">bf16-true</code></td> <td>Fastest</td> <td>Medium</td> <td>No</td> <td>A100, TPUs</td> </tr> </tbody> </table> <hr> <h2 id="learning-rate-schedulers">Learning Rate Schedulers</h2> <p>Tip: (TODO: from personal experience and advice from Andy and other model (clay)): Use cosine annealing with warm restarts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">CosineAnnealingWarmRestarts</span>
</code></pre></div></div> <p>Cosine annealing with warm restarts is effective because it:</p> <ol> <li> <strong>Escapes local minima</strong>: The periodic “restarts” help the model escape poor local minima by temporarily increasing the learning rate.</li> <li> <strong>Faster convergence</strong>: The cyclical learning rate schedule often leads to faster convergence than linear or step decay schedules.</li> <li> <strong>Improved generalization</strong>: Research shows this approach often leads to better generalization performance on test data.</li> <li> <strong>Parameter exploration</strong>: The warm restarts enable broader exploration of the parameter space initially, while still allowing fine-tuning later.</li> <li> <strong>Works across domains</strong>: This approach has proven effective across various model architectures and tasks.</li> </ol> <p>As a general strategy, it’s hard to beat and typically performs better than fixed or step decay schedules with minimal tuning.</p> <hr> <h3 id="environment-variables--and-why-theyre-nice">Environment Variables — And Why They’re Nice</h3> <p>Env vars help you <strong>manage secrets and config</strong> without hardcoding.<br> Why it’s nice:</p> <ul> <li>Store keys/passwords securely</li> <li>Change behavior across environments (dev, prod, test)</li> <li>Fast configuration without changing code</li> </ul> <p>Here’s a simple <code class="language-plaintext highlighter-rouge">.env</code> file example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Cloud storage credentials
CLOUD_STORAGE_KEY=your_secret_key_here
CLOUD_STORAGE_BUCKET=training-data-bucket

# Experiment tracking
WANDB_API_KEY=your_wandb_key_here
EXPERIMENT_NAME=transformer_v2
</code></pre></div></div> <p><strong>Important</strong>: Never commit your <code class="language-plaintext highlighter-rouge">.env</code> file to your repository. It often contains sensitive information like API keys and credentials. Instead:</p> <ol> <li>Add <code class="language-plaintext highlighter-rouge">.env</code> to your <code class="language-plaintext highlighter-rouge">.gitignore</code> file</li> <li>Provide a <code class="language-plaintext highlighter-rouge">.env.example</code> template with dummy values in the repo</li> <li>Document the required environment variables in your README</li> <li>For cloud deployments, use the platform’s secrets management (AWS Secrets Manager, GitHub Secrets, etc.)</li> </ol> <p>This approach keeps your secrets secure while making configuration straightforward for team members.</p> <hr> <h2 id="pre-training-strategy-experiment-first">Pre-training Strategy: Experiment First</h2> <p>Before jumping into full-scale training:</p> <ul> <li>Select a baseline configuration</li> <li>Observe the loss curves over epochs</li> <li>Run small-scale experiments to test different settings</li> <li>Compare outcomes to identify the best setup</li> </ul> <p>This approach helps avoid wasting compute and accelerates convergence.</p> <p>TODO: add a picture of the loss curves and explain some examples</p> <h2 id="whats-after-this-guide">What’s After This Guide?</h2> <h3 id="faster">Faster</h3> <ul> <li>Look into <strong>JAX</strong> for speed and flexibility</li> </ul> <h3 id="bigger">Bigger</h3> <ul> <li>Model parallelism, sharding, and memory optimization techniques like: <ul> <li> <strong>Zero Redundancy Optimizer (ZeRO)</strong><br> <a href="https://oracle-oci-ocas.medium.com/zero-redundancy-optimizers-a-method-for-training-machine-learning-models-with-billion-parameter-472e8f4e7a5b" rel="external nofollow noopener" target="_blank">Read more here</a> </li> </ul> </li> </ul> <p>These are advanced topics — no need to rush. Focus on working with the current tools first.</p> </body></html>