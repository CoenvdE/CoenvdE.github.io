<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>layout: external_blog title: ‘3. Optimizing The Pipeline: Data’ description: Part of the Training at Scale series date: ‘2025-05-05’ collection: external_blogs collection_id: training_at_scale chapter_number: 4 source_repo: https://github.com/CoenvdE/Training-at-larger-scale-blog original_file: “/3. Optimizing the pipeline: Data.md”</p> <hr> <h2 id="3-optimizing-the-pipeline-data">3. Optimizing the pipeline: Data</h2> <p>Efficient data loading can significantly reduce training time — especially when the GPU is fast but starved for data.</p> <ul> <li>The GPU (or TPU/HPU etc.) is <strong>the most expensive and performance-critical component</strong> in a training pipeline.</li> <li>If the data pipeline (i.e., loading, preprocessing, transferring) is slow, the GPU will sit idle, waiting for the next batch.</li> <li>The goal is to <strong>saturate the GPU</strong> — keep it busy with minimal idle time.</li> </ul> <h3 id="what-you-want">What You Want</h3> <ul> <li>A data pipeline <strong>that saturates the GPU as much as possible</strong> without introducing to much memory pressure or overdemanding CPU’s.</li> <li>Each DataLoader worker is effectively a producer that loads data in parallel, while your training loop (running on the main process/GPU) is the consumer. The goal is to maximize overlap: while one batch is being processed on the GPU, the next batch (or several batches) are being loaded by the CPU workers in the background</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3. Optimizing the pipeline: Data/
├── benchmark_datapipeline_configurations.py  # Main benchmark script
├── plot_benchmark_runs.ipynb                # Analysis notebook
├── dummydataset.py                          # Dummy dataset for testing
├── utils.py                                 # Utility functions
├── benchmark_logs/                          # Benchmark results
└── benchmark_imgs/                          # Visualizations
</code></pre></div></div> <h3 id="key-concepts-and-metrics">Key Concepts and Metrics</h3> <hr> <ul> <li> <p><strong>CPU (CPU)</strong>:</p> <ul> <li>Used by the <strong>main process and workers</strong> to (lazy) load, transform, and prepare data for training.</li> <li>Ideally, you want high CPU usage (to indicate workers are busy) but not so high that the OS or other processes starve. If your CPU utilization is peaking at 100% on all cores and the system becomes unresponsive or the throughput plateaus, you may have too many workers or threads.</li> <li>You can check available CPUs using:</li> </ul> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="nf">print</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">())</span>
</code></pre></div> </div> </li> <li> <p><strong>RAM (System Memory / CPU RAM)</strong>:</p> <ul> <li>Used by workers to load batches into memory.</li> <li>Important when working with large datasets, large batch sizes, or complex transforms.</li> <li>Too many workers without enough RAM can lead to crashes or swapping, which hurts performance.</li> </ul> </li> <li> <p><strong>GPU Saturation</strong>:</p> <ul> <li>The key performance target.</li> <li>If GPU is underutilized, bottleneck is likely in data loading.</li> </ul> </li> <li> <p><strong>I/O Considerations</strong></p> <ul> <li> <p><strong>Bandwidth</strong>: The maximum rate of data transfer between components in your system:</p> <ul> <li>Disk/cloud storage to RAM</li> <li>CPU memory to GPU memory</li> <li>Network transfers (e.g., from cloud storage to your instance)</li> </ul> <p>Measured in MB/s or Gbps. High bandwidth allows more data to flow per second, while low bandwidth creates bottlenecks that can leave your GPU waiting for data. When working with cloud data, network bandwidth often becomes the primary constraint. Cloud servers typically have significantly higher bandwidth (often 10-100x faster) than consumer (wifi) internet connections, which can dramatically improve data transfer speeds when training in cloud environments compared to local setups.</p> </li> <li> <strong>Chunking</strong>: Use optimal chunk sizes to balance I/O overhead and memory usage</li> <li> <strong>Optimizations</strong>: <ul> <li>Memory mapping for large files</li> <li>Prefetching to hide latency</li> <li>Caching frequently used data</li> <li>Sequential access over random reads</li> <li>Compression to reduce I/O</li> </ul> </li> <li> <strong>File Formats</strong>: Choose ML-optimized formats (Parquet, TFRecord, WebDataset, Zarr)</li> </ul> </li> </ul> <p><strong>Streaming Workers (Dask) vs DataLoader Workers (PyTorch)</strong></p> <p>PyTorch DataLoader and streaming frameworks like Dask can be combined effectively to stream cloud data into your training pipeline, but it’s crucial to understand they operate at different layers of the data process:</p> <ul> <li> <p><strong>Dask (or other streaming frameworks)</strong> handles the low-level data access by:</p> <ul> <li>Building a computational graph (DAG) for lazy evaluation</li> <li>Managing how data chunks are read from storage</li> <li>Orchestrating parallel fetching of data from cloud/disk</li> <li>Optimizing memory usage through chunking strategies</li> </ul> </li> <li> <p><strong>PyTorch DataLoader</strong> manages the training-specific data handling by:</p> <ul> <li>Creating worker processes to parallelize sample preparation</li> <li>Implementing batching, shuffling, and sampling strategies</li> <li>Executing the preprocessing defined in your <code class="language-plaintext highlighter-rouge">__getitem__</code> method</li> <li>Transferring prepared batches to the GPU</li> </ul> </li> </ul> <p>These systems don’t automatically coordinate with each other. The connection point occurs when a DataLoader worker calls <code class="language-plaintext highlighter-rouge">__getitem__</code> on your Dataset, which then triggers Dask to materialize the required data chunks when calling a <code class="language-plaintext highlighter-rouge">.load()</code> function. By understanding this separation, you can optimize each layer independently:</p> <ol> <li>Configure optimal data reading (e.g. chunk sizes, thread count, worker count)</li> <li>Configure DataLoader for optimal batch preparation (e.g. num_workers, prefetch_factor)</li> </ol> <p>It’s important to avoid conflicting parallelism between these systems because too many concurrent processes and threads can lead to resource contention and degraded performance.</p> <h3 id="practical-guidelines-background-and-setup">Practical guidelines, background and setup</h3> <hr> <p><strong>Understanding DataLoader Workers and Process Distribution</strong></p> <p>When optimizing your data pipeline, it’s important to understand how worker processes are distributed across your available CPU cores and to leave sufficient CPU resources for main processes and system overhead.</p> <p>Reserve 1-2 CPU cores per GPU for the main training process and system overhead. When using containerization solutions like Docker, you may need to reserve additional CPU resources for container management.</p> <ul> <li> <p><strong>Single GPU setup</strong>: With a single GPU and one main training process, you’ll have a single DataLoader that spawns <code class="language-plaintext highlighter-rouge">num_workers</code> processes. For example, on a machine with 20 CPU cores and 1 GPU, you should reserve 1-2 cores for the main process and system overhead, leaving about 18 cores for DataLoader workers.</p> </li> <li> <p><strong>Multi-GPU setup</strong>: With multiple GPUs (e.g., using DistributedDataParallel), each GPU has its own main process, and each main process creates its own set of DataLoader workers. For instance, on a machine with 8 GPUs and 160 CPU cores, you should reserve 8-16 cores for main processes and system overhead (1-2 per GPU), leaving approximately 144-152 cores to be distributed among all worker processes.</p> </li> </ul> <p>To avoid CPU contention, ensure the total number of worker processes doesn’t exceed your available CPU cores minus the cores needed for main processes and system overhead. When workers compete for CPU resources, data loading becomes inefficient and can slow down training.</p> <p><strong>CPU allocation examples</strong>:</p> <ul> <li>20 CPUs, 1 GPU: Reserve 2 CPUs for main process and overhead, use up to 18 workers</li> <li>60 CPUs, 3 GPUs: Reserve 6 CPUs (2 per GPU) for main processes and overhead, use up to 18 workers per GPU (54 total)</li> </ul> <p>Again, note that num_workers is not about CPU’s but processes, and a process may use more or less then 1 cpu core.</p> <p><strong>Set batch size</strong> The ideal batch size depends on your dataset characteristics, model complexity, and available GPU memory. You want a size that’s stable but still provides stochastic gradient descent benefits. For my use case, 32 worked well, but you may need to adjust based on your specific requirements. This batch size is needed because it is used in my optimization benchmark script later.</p> <p><strong>Optional: measure time for 1 batch to train</strong> Measuring the time it takes to (load a minibatch and) complete a single training step. This information is useful for properly configuring the benchmark script parameters to accurately reflect your real-world training conditions. For this, you can run the <code class="language-plaintext highlighter-rouge">timing_benchmark.py</code> script that is in the folder of <a href="4.%20Optimizing%20the%20pipeline%3A%20Model.md">chapter 4</a>.</p> <p><strong>No need Over-optimize the DataLoader</strong>: If your model is small or the GPU is not very powerful, there’s no point in using 16+ workers or heavy parallel jobs when the GPU is already saturated. I have spend quite some time doing research on this, and it is imporatant to think about this step as it can drastically increase your performance, but no need to do “gridsuch-like stuff for this. Follow my guide and your speed should already improve a lot. Experimenting endlessly with this also costs money and potential experimentation time. Ill tell you more on how to do it in a sec.</p> <h3 id="when-optimizing-data-loading-identify-the-bottleneck-using-the-benchmark-script-i-created-look-for-these-key-indicators">When optimizing data loading, identify the bottleneck using the benchmark script I created. Look for these key indicators:</h3> <hr> <p>• <strong>GPU Under-utilization</strong>: If your GPU is frequently idle, your data pipeline isn’t keeping up. The benchmark will show print long waits between training steps and white spaces in the “train row” of the visualization.</p> <p>• <strong>Single CPU core maxed out</strong>: If one CPU core is at 100% while others are idle (especially with num_workers=0), increase num_workers to distribute the load across multiple cores.</p> <p>• <strong>All CPU cores busy but GPU still waiting</strong>: This suggests an I/O bottleneck (slow disk, network, etc.). Adding more workers won’t help. Instead, optimize at the dataset level with faster storage, caching, or better chunking strategies.</p> <h3 id="what-can-be-optimized">what can be optimized</h3> <hr> <p><strong><em>dataset</em></strong></p> <p>This part is not always needed. If there are no parameters to be optimized, chunking to be optimized or file format to be optimized, focus on optimizing the dataloader. (when streaming from machine’s disk memory, ssd or when streaming is all handled automatically)</p> <p><strong>what to optimize</strong></p> <ol> <li> <strong>Efficient Storage Formats</strong>:</li> </ol> <p>Initially, my data was stored in NetCDF files, which are common for scientific data but can be inefficient when working with streaming from cloud storage. The default chunking in these files was not optimized for machine learning, causing unnecessary data to be loaded into memory during training. To address this, I converted everything to <a href="https://zarr.readthedocs.io/" rel="external nofollow noopener" target="_blank">Zarr format</a>. Zarr is specifically designed for cloud-based data access. I will not include the migration in this blog, as it is out of scope, I just want to show that it is important to think about the dataformats used.</p> <p>TODO: zarr threading fast</p> <ol> <li> <strong>Optimal Chunking</strong>:</li> </ol> <p><strong><em>How to calculate (handwavy) how big your chunks should be</em></strong></p> <p>TODO: LAURENS CALCULATIONS When migrating to Zarr, I defined chunk sizes …</p> <ol> <li><strong>Parallelize I/O: Loading Efficiently</strong></li> </ol> <p>Some libraries, like Dask, can parallelize reading within a dataset, providing their own optimization parameters. However, be careful when combining these with PyTorch DataLoader workers, as this can lead to resource contention and diminishing returns.</p> <p>When working with lazy-loading/streaming data into GPU memory, there are several parameters you might need to optimize:</p> <ol> <li> <p><strong>Parallel Reading Parameters</strong>:</p> <ul> <li>Number of concurrent readers/workers</li> <li>Memory limits per worker</li> <li>TODO: zarr threading fast</li> </ul> </li> <li> <p><strong>Caching Parameters</strong>:</p> <ul> <li>Cache size limits</li> <li>Cache eviction policies</li> <li>Persistent vs. in-memory caching</li> </ul> </li> </ol> <p>In my specific case with geospatial data using Dask, I needed to optimize:</p> <ul> <li>Number of Dask workers (processes that execute computations)</li> <li>Memory limit per Dask worker (to prevent OOM errors)</li> <li>Thread pool size per worker (for parallel execution within a worker)</li> <li>TODO: zarr threading fast</li> </ul> <p><strong>Usecase: Understanding Dask Execution Modes</strong></p> <p>Dask offers three execution modes, each with different trade-offs:</p> <ol> <li> <p><strong>Single-Threaded Scheduler</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">dask.config.set(scheduler="single-threaded")</code> </li> <li>All tasks run sequentially in the main thread</li> <li>No parallelism but minimal overhead</li> <li>Useful for debugging or when relying entirely on DataLoader’s parallelism</li> <li>In multi-GPU setups, each process runs its own sequential scheduler</li> </ul> </li> <li> <p><strong>Threaded Scheduler</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">dask.config.set(scheduler="threads", num_workers=dask_threads)</code> </li> <li>Tasks run in parallel using a thread pool within a single process</li> <li>Good for I/O-bound operations (like reading data chunks)</li> <li>Moderate parallelism with low overhead</li> <li>In multi-GPU setups, be careful of the total thread count (e.g., 8 processes × 4 threads = 32 threads)</li> </ul> </li> <li> <p><strong>Distributed Cluster</strong></p> <ul> <li>Uses <code class="language-plaintext highlighter-rouge">LocalCluster</code> and <code class="language-plaintext highlighter-rouge">Client</code> to create a full Dask cluster</li> <li>Runs multiple worker processes, each with multiple threads</li> <li>Provides process-level parallelism, bypassing Python’s GIL</li> <li>Includes a dashboard for monitoring tasks and resource usage</li> <li>Options for per-GPU-process clusters or a single shared cluster</li> <li>Higher overhead but better isolation and monitoring capabilities</li> </ul> </li> </ol> <p>For a single GPU with limited CPUs (e.g., 20 cores):</p> <ul> <li>A threaded scheduler with 4-8 threads is often sufficient</li> <li>A small distributed cluster (1-2 workers, 4 threads each) offers better monitoring</li> </ul> <p>For multi-GPU setups (e.g., 8 GPUs with 20 cores):</p> <ul> <li>Be careful not to oversubscribe your CPU resources</li> <li>If each GPU process uses its own Dask cluster, limit to 1 worker with 2 threads per process</li> <li>Consider using a single shared Dask cluster for all GPU processes</li> <li>Monitor CPU utilization to avoid contention</li> </ul> <p>The key is balancing parallelism against resource constraints. More parallelism isn’t always better, especially when resources are shared across multiple GPU processes. Start conservative and scale up while monitoring performance. TODO: needed?</p> <p>TODO: zarr threading fast</p> <ol> <li> <strong>Caching and Locality</strong>: For remote data, implement caching strategies to avoid repeatedly downloading the same data. Consistent access patterns help leverage OS-level caching.</li> </ol> <p>If your data pipeline doesn’t use these specialized libraries, you can focus solely on DataLoader optimization.</p> <p>In summary, dataset-level optimization is about making data access as efficient as possible. By storing data smartly (chunked, compressed appropriately, possibly colocated with training if remote), and by only doing minimal necessary work for each access, you ensure that the raw data supply is fast. Once that is in place, DataLoader-level tuning can further amplify the throughput.å</p> <p><strong><em>dataloader</em></strong></p> <p><strong>what to optimize</strong> <strong>Parameters to Optimize in the DataLoader</strong></p> <p>The DataLoader is critical for training performance. Key parameters to optimize:</p> <ol> <li> <p><strong><code class="language-plaintext highlighter-rouge">num_workers</code></strong>: Controls parallel data loading subprocesses</p> <ul> <li>Too few: GPU waits for data</li> <li>Too many: Resource contention, diminishing returns</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">prefetch_factor</code></strong>: Batches loaded in advance per worker</p> <ul> <li>Default is 2, which works for most cases</li> <li>Adjust based on sample size and memory requirements</li> <li>Total prefetched = <code class="language-plaintext highlighter-rouge">num_workers * prefetch_factor</code> </li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">pin_memory</code></strong>: Enables faster CPU to GPU transfers</p> <ul> <li>Set to <code class="language-plaintext highlighter-rouge">True</code> when using GPU</li> <li>Creates page-locked memory for direct transfers</li> <li>Slightly increases CPU memory usage</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">persistent_workers</code></strong>: Keeps workers alive between epochs</p> <ul> <li>Reduces worker initialization overhead</li> <li>Useful for large datasets and complex initialization</li> <li>Significantly reduces epoch transition time</li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">multiprocessing_context</code></strong>: Controls worker process spawning</p> <ul> <li>Options: ‘fork’, ‘spawn’, ‘forkserver’</li> <li>Use ‘forkserver’ (or ‘spawn’) for better CUDA compatibility</li> </ul> </li> </ol> <p>Even with an efficient dataset, proper DataLoader settings are crucial.</p> <h3 id="optimizing-benchmarking-tools-for-dataloader-optimization">Optimizing: Benchmarking Tools for DataLoader Optimization</h3> <p>TODO: discuss with laurens if I should add andy’s script here as well.</p> <p>I’ve created two files to help you optimize your data pipeline:</p> <ol> <li> <p><strong><code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code></strong></p> <ul> <li>suitable for any pytorch Dataset object</li> <li>Runs and logs different DataLoader configurations</li> <li>Measures performance metrics for each configuration</li> <li>Includes Dask integration (commented out by default for those who don’t need it)</li> <li>Contains a configurable <code class="language-plaintext highlighter-rouge">train_step_time</code> parameter (default: 0.1 seconds) <ul> <li>This simulates model training time</li> <li>You can set this to match your actual model’s training time per batch</li> <li>The goal remains the same: minimize data loading wait times</li> </ul> </li> </ul> </li> <li> <p><strong><code class="language-plaintext highlighter-rouge">plot_benchmark_runs.ipynb</code></strong></p> <ul> <li>Jupyter notebook for visualizing benchmark results</li> <li>Creates charts comparing different configurations</li> <li>Helps identify the optimal setup for your specific hardware</li> </ul> </li> </ol> <p>If you’re using Dask, you can also leverage the Dask dashboard to monitor:</p> <ul> <li>Worker memory usage</li> <li>CPU utilization</li> <li>Task execution</li> <li>Resource bottlenecks</li> </ul> <h3 id="example-benchmark-results">Example Benchmark Results</h3> <p>Below are example benchmark results showing the dramatic difference between an unoptimized baseline configuration and an optimized one:</p> <div style="display: flex; justify-content: space-between; margin-bottom: 20px;"> <div style="flex: 1; margin-right: 10px;"> <img src="images/benchmark_logs_bad_baseline.png" alt="Unoptimized Baseline" style="width: 100%;"> <p><em>Unoptimized baseline configuration with significant data loading bottlenecks</em></p> </div> <div style="flex: 1; margin-left: 10px;"> <img src="images/benchmark_logs_good_baseline.png" alt="Optimized Configuration" style="width: 100%;"> <p><em>Optimized configuration with minimal wait time</em></p> </div> </div> <p>With these simple optimizations, my dataloader became 5 times faster than the baseline, the company that made the first version of this benchmark script was able to run 15x faster. This performance improvement is transformative for training - where others might only complete 50 epochs in a given timeframe, I was able to run 250 epochs. This acceleration dramatically reduces overall training time and allows for more experimentation and model iterations.</p> <h3 id="practical-optimization-approach">Practical Optimization Approach</h3> <hr> <ol> <li><strong>Import your own dataset at the start</strong></li> </ol> <ul> <li>Follow these steps to systematically optimize your data pipeline: change the line on top of <code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dummydataset</span> <span class="kn">import</span> <span class="n">DummyDataset</span> <span class="k">as</span> <span class="n">YourDataset</span> <span class="c1"># replace with your dataset
</span></code></pre></div></div> <ol> <li> <p><strong>Establish a baseline</strong></p> <ul> <li>Start with minimal configuration: <ul> <li> <code class="language-plaintext highlighter-rouge">num_workers = 0</code> (single-process loading)</li> <li> <code class="language-plaintext highlighter-rouge">prefetch_factor = None</code> (default behavior)</li> <li><code class="language-plaintext highlighter-rouge">persistent_workers = False</code></li> <li><code class="language-plaintext highlighter-rouge">pin_memory = False</code></li> </ul> </li> <li>This provides a reference point for measuring improvements</li> </ul> </li> <li> <p><strong>Implement a sensible default configuration</strong></p> <ul> <li>For a system with 10 CPU cores and 1 GPU: <ul> <li>Reserve 1-2 cores for the main training process</li> <li>Allocate remaining cores between DataLoader workers and streaming processes</li> <li>Example allocation: <ul> <li>5 cores for Dask streaming workers (if using Dask)</li> <li>8 cores for DataLoader workers</li> </ul> </li> <li>Note: Some CPU oversubscription (e.g., 13 workers on 10 cores) can be beneficial <ul> <li>When workers are waiting for I/O operations, they don’t use CPU</li> <li>This can increase overall throughput by reducing idle time</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><strong>Experiment with different configurations</strong></p> <ul> <li>Test variations systematically: <ul> <li>More streaming workers, fewer DataLoader workers</li> <li>More DataLoader workers, fewer streaming workers</li> <li>Higher CPU oversubscription</li> <li>Lower CPU oversubscription</li> <li>Different <code class="language-plaintext highlighter-rouge">prefetch_factor</code> values</li> </ul> </li> </ul> </li> <li> <p><strong>Run benchmarks and analyze results</strong></p> <ul> <li>Use <code class="language-plaintext highlighter-rouge">benchmark_configurations.py</code> to test all configurations</li> <li>Can be run locally or in the cloud</li> <li>(Download and) analyze the files with <code class="language-plaintext highlighter-rouge">plot_benchmark_runs.ipynb</code> (use the <code class="language-plaintext highlighter-rouge">.log</code> files, not the <code class="language-plaintext highlighter-rouge">_workers.log</code> files)</li> <li>Compare performance metrics across configurations</li> </ul> </li> </ol> <p>Use the plotting notebook to visualize differences between runs and identify which configuration has the lowest wait/batch fetching time. While I don’t explicitly measure vRAM/CPU utilization in these tools (as it’s complex and time-consuming to implement), the primary goal is to significantly improve training time with reasonable effort. Having that said, watch for these warning signs:</p> <ul> <li> <p><strong>Memory issues</strong>: If memory usage spikes, reduce workers, prefetch factor, or caching.</p> </li> <li> <p><strong>Diminishing returns</strong>: If adding workers doesn’t improve throughput, you’ve reached saturation.</p> </li> <li> <p><strong>Memory leaks</strong>: If RAM usage continuously grows, check for Dask worker caching, large objects in persistent workers, or reference issues.</p> </li> </ul> <h3 id="cloud-vs-local-performance">Cloud vs. Local Performance</h3> <p>Note that network bandwidth varies dramatically between environments. When moving from local development (WiFi) to cloud training, you may see orders of magnitude improvement in data loading speed. In my case, I observed a 100x decrease in wait time when moving to the cloud. Always benchmark in the same environment where you’ll be training, as the optimal configuration can differ significantly between local and cloud setups.</p> <p>For more insights on optimizing cloud data loading, see <a href="https://earthmover.io/blog/cloud-native-dataloader/" rel="external nofollow noopener" target="_blank">Earthmover’s guide to cloud-native dataloaders</a> covering streaming techniques, I/O optimization, and resource balancing.</p> <p>Now that we have the data-part of the pipeline optimized, lets focus on the <a href="4.%20Optimizing%20the%20pipeline%3A%20Model.md">Model</a></p> </body></html>