<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>layout: external_blog title: 0. The Setup description: Part of the Training at Scale series date: ‘2025-05-05’ collection: external_blogs collection_id: training_at_scale chapter_number: 1 source_repo: https://github.com/CoenvdE/Training-at-larger-scale-blog original_file: “/0. The Setup.md”</p> <hr> <h2 id="0-the-setup">0. The Setup</h2> <p>Before diving into optimizations, our starting point needs to be solid. I will walk you through some best practices and a baseline to give you a solid starting point. Note that my model and pipeline were actually a lot more complicated, but my goal is not how to recreate my complicated pipeline, it is to show you simple examples for you to optimize and improve yours.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0. The Setup/
├── config/
│   └── config.yaml
├── output/
├── src/
│   ├── data/
│   │   ├── __pycache__/
│   │   └── pytorch_dataset.py
│   └── model/
│       ├── __pycache__/
│       ├── pytorch_model.py
│       ├── pytorch_encoder.py
│       └── pytorch_decoder.py
├── tests/
│   ├── __pycache__/
│   ├── __init__.py
│   └── test_parameters.py
├── wandb/
├── pytorch_train.py
└── requirements.txt
</code></pre></div></div> <h3 id="my-model-components-and-overview">My Model (components and overview)</h3> <hr> <p>My model was essentially a (Masked) Autoencoder (with some cool stuff that does not matter for this guide) and had the following key components:</p> <ul> <li><code class="language-plaintext highlighter-rouge">pytorch_encoder</code></li> <li><code class="language-plaintext highlighter-rouge">pytorch_decoder</code></li> <li> <code class="language-plaintext highlighter-rouge">pytorch_model</code> – responsible for the forward pass and loss calculation (plus some additional components that aren’t essential for this guide.)</li> </ul> <h3 id="my-dataset">My Dataset</h3> <hr> <ul> <li>a <code class="language-plaintext highlighter-rouge">pytorch_dataset</code><br> I used a big, geospatial dataset for my training. For this tutorial, I created a dummy example, but feel free to swap it out for e.g. <a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="external nofollow noopener" target="_blank">CIFAR</a> or any other dataset you like. It is important that this is wrapped into a <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> object.<br> We will come back to data stuff in more detail in <a href="2.%20Bigger%20data%20in%20the%20cloud.md">Chapter 2</a>.<br> Note that I created a generator for the dataloader that has a set seed and I also used a <code class="language-plaintext highlighter-rouge">set_seed</code> function to keep everything reproducible in the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file.</li> </ul> <h3 id="reproducibility">Reproducibility</h3> <hr> <p>Reproducibility is very important, it allows you and others to reliably verify and compare results. In research, it ensures findings are valid and consistent. For practitioners, it makes debugging and iterative experimentation much easier. Seeding is very important for reproducibility.<br> you need to seed everything (torch, numpy, python, etc.). Look at the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file for an example.<br> Seed not properly propagated to data-generators, so I created a generator for the dataloader that has a set seed and I also used a <code class="language-plaintext highlighter-rouge">set_seed</code> function to keep everything reproducible in the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file. Pytorch dataloader workers (I will properly explain everything this in <a href="3.%20Optimizing%20the%20pipeline%20-%20Data.md">Chapter 3</a>) need to be seeded, because each worker runs in its own process, and without explicit seeding, they will use random seeds, leading to non-deterministic data loading and augmentations. look at the <code class="language-plaintext highlighter-rouge">worker_init_fn</code> in the <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> file for an example.</p> <p>For full reproducibility (usefull for debugging and testing), you should also set the following: - <code class="language-plaintext highlighter-rouge">torch.backends.cudnn.deterministic = True</code> # slows down training but makes results reproducible (default is False) - <code class="language-plaintext highlighter-rouge">torch.backends.cudnn.benchmark = False</code> # slows down training but makes results reproducible (default is True) These settings control how CUDA kernels are selected and run. Setting deterministic = True ensures the same operations produce the same outputs every run, at the cost of performance. Disabling benchmark prevents the selection of non-deterministic, optimized algorithms that could vary between runs. torch.backends.cudnn.benchmark = False disables the auto-tuner that selects the fastest convolution algorithms based on input sizes. While this can speed up training, it introduces non-determinism when input sizes vary. Setting it to False ensures consistent behavior across runs.</p> <h3 id="reproducibility-1">Reproducibility</h3> <hr> <p>Reproducibility is essential — it allows you and others to reliably verify and compare results.<br> In research, it ensures findings are valid and consistent. For practitioners, it makes debugging and iterative experimentation much easier.</p> <p>A key part of reproducibility is proper seeding. You need to seed <strong>everything</strong>:</p> <ul> <li>Python’s built-in <code class="language-plaintext highlighter-rouge">random</code> </li> <li>NumPy</li> <li>PyTorch</li> <li>Everything else that needs a seed</li> </ul> <p>See the <code class="language-plaintext highlighter-rouge">set_seed</code> function in <code class="language-plaintext highlighter-rouge">pytorch_train.py</code> for an example.</p> <p>By default, PyTorch <code class="language-plaintext highlighter-rouge">DataLoader</code> workers (A full explanation is provided in <a href="3.%20Optimizing%20the%20pipeline%20-%20Data.md">Chapter 3</a>) are not seeded properly — each worker runs in its own process, and without explicit seeding, they will each get a random seed. This can lead to <strong>non-deterministic data loading and augmentations</strong>.</p> <p>To fix this, create a generator for the <code class="language-plaintext highlighter-rouge">DataLoader</code> with a fixed seed and use a <code class="language-plaintext highlighter-rouge">worker_init_fn</code>.<br> Check out the examples in <code class="language-plaintext highlighter-rouge">pytorch_train.py</code>.</p> <p>For full reproducibility — especially useful during debugging and testing — set the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># Default: False
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="bp">False</span>     <span class="c1"># Default: True
</span></code></pre></div></div> <p>These flags control how CUDA kernels are selected and run: • deterministic = True ensures that operations produce the same results across runs, but may slow training. • benchmark = False disables CuDNN’s auto-tuner, which usually picks the fastest convolution algorithms. While this can speed up training, it may introduce non-determinism, especially when input sizes vary.</p> <p>Setting both ensures consistent, repeatable behavior across runs.w</p> <h3 id="config-files">Config files</h3> <hr> <p>I use config files for the model, dataloader, training, optimizer, and scheduler arguments. This is best practice, allowing quick adjustments to hyperparameters without modifying the code. This makes experimentation and testing more efficient.<br> An example of a config file with a few of these parameters is given in:<br> <code class="language-plaintext highlighter-rouge">Training-at-larger-scale-blog/0. The Setup/config/config.yaml</code></p> <h3 id="unittests">Unittests</h3> <hr> <p>Before moving forward, ensure your model actually works. It is important to write tests that check:</p> <ul> <li>Shape consistency</li> <li>Correct device allocation</li> <li>Other problem specific stuff that you might need</li> <li>Proper parameter loading from config file</li> </ul> <p>This last one is very important and can save you a lot of trouble debugging, I made an example test for this in my <code class="language-plaintext highlighter-rouge">.tests</code> folder</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m unittest tests/test_parameters.py
</code></pre></div></div> <p>Additionally, I set up automated testing — every push to the GitHub branch runs these tests.<br> For this, look at the <code class="language-plaintext highlighter-rouge">.github/workflows/test.yml</code> file.<br> While unit tests don’t catch everything, they help ensure the code runs smoothly (in the cloud) and prevent accidental breakage.<br> Automatically running them via GitHub gives you good feedback on if you break anything!</p> <h3 id="tracking--experiment-logging-wandb">Tracking &amp; Experiment Logging (WandB)</h3> <hr> <p>Proper tracking is essential for monitoring model performance and debugging issues.<br> I used Weights &amp; Biases (WandB), but alternatives like MLflow also work.</p> <p>What to track?</p> <ul> <li>Validation loss &amp; training loss</li> <li>Learning rate</li> <li>Reconstructions &amp; visualizations to monitor model progress</li> <li>Config files &amp; training logs for reproducibility</li> <li>Other problem specific stuff</li> </ul> <p>You can decide for yourself what you want to track and when (per step, per epoch), it never hurts to track more things!<br> Give the training loop a go to see what happens:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python pytorch_train.py
</code></pre></div></div> <h3 id="validating-my-architecture">Validating My Architecture</h3> <hr> <p>The problem I needed to solve was a geospatial one. This means that my model needs to be capable of understanding multiple dimensions:<br> variables (channels), height (latitude), width (longitude) and time (month).</p> <p>Before I moved on to experimenting with my big geospatial dataset, I checked if my model was able to train on local hardware (and a single GPU).<br> I validated my model architecture on the following datasets:</p> <ul> <li>A Dummy Dataset (randomly initialized tensors with the correct shapes): to see if the model worked with input as expected. The training signal itself does not tell you much, but it sets up the pipeline correctly.</li> <li>CIFAR-10: While CIFAR-10 isn’t my actual problem, it served as a good check for my problem as the dataset has height and width dimensions, 3 variable dimensions (RGB) and a “time” dimension (time = 1 always, but it works with the model architecture)</li> <li>A small subset of my “big” geospatial dataset.</li> </ul> <hr> <p>Once my model was working, validated and tracking was set up, it was time to dive into how to make it efficient and suitable for <a href="1.%20Multi-GPU%20training.md">larger scale training with multiple GPUs</a></p> </body></html>